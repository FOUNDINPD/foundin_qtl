{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0f7991-1142-46d8-bda9-2cd035499934",
   "metadata": {},
   "source": [
    "## Notebook for scan the public Bryois et al Brain cell-type eQTL results\n",
    "\n",
    "- [Bryois et. al.](https://pubmed.ncbi.nlm.nih.gov/35915177/)\n",
    "    - Bryois J, Calini D, Macnair W et al. Cell-type-specific cis-eQTLs in eight human brain cell types identify novel risk genes for psychiatric and neurological disorders. Nat Neurosci 2022;25:1104â€“12.\n",
    "    - 192 individuals, snRNA, 8 CNS cell-types, multiple cohorts, mostly DLPFC, but also temporal cortex and deep white matter\n",
    "    - downloaded files are full fastQTL results per cell-type per autosome\n",
    "    - provided snp_pos file that includes allele info\n",
    "    - fastQTL only outputs p-values and betas (coefficients)\n",
    "    \n",
    "Scan the Bryois results for any nominal intersect with PD risk and for these intersects then run colocalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a353ef4e-7149-4d4d-89eb-b98cd11868f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 17 18:14:47 UTC 2023\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bac636-3f2b-456c-94a1-7de18dac947d",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a99407-d70e-4570-831f-1b2bb7adf751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame, Series, merge, concat\n",
    "from threading import Thread\n",
    "from numpy import around\n",
    "import colocalization as clc\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53578c-5c3b-479b-ad98-348a38de95c9",
   "metadata": {},
   "source": [
    "#### set notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1f7f20-1ab4-4e9e-bbd1-2ec68b19a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming\n",
    "set_name_frmt = 'foundin_daNA_Bryois-{cell_abbrv}'\n",
    "dx = 'PD'\n",
    "\n",
    "# directories \n",
    "wrk_dir = '/labshare/raph/datasets/foundin_qtl'\n",
    "public_dir = f'{wrk_dir}/public'\n",
    "results_dir = f'{wrk_dir}/results'\n",
    "\n",
    "# input files\n",
    "# with agreement in place use full summary stats\n",
    "# gwas_sum_stats_file = f'{public_dir}/nalls_pd_gwas/pd_sumstats_no23andme_buildGRCh38.tsv.gz'\n",
    "gwas_sum_stats_file = f'{public_dir}/nalls_pd_gwas/pd_sumstats_23andme_buildGRCh38.tsv.gz'\n",
    "index_variants_file = f'{public_dir}/nalls_pd_gwas/index_variants.list'  \n",
    "\n",
    "# variables\n",
    "DEBUG = False\n",
    "max_nominal = 0.01\n",
    "num_qtl_samples = 192\n",
    "cell_names_dict = {'Astrocytes': 'Astro', 'Endothelial.cells': 'Endo', \n",
    "                   'Excitatory.neurons': 'ExN', 'Inhibitory.neurons': 'InN', \n",
    "                   'Microglia': 'Micro', 'OPCs...COPs': 'OPC', \n",
    "                   'Oligodendrocytes': 'Oligo', 'Pericytes': 'Peri'}\n",
    "min_h4 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5415c7-ec76-4b37-8493-e5f1f913213b",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a55ed5-ef3c-4eef-9a7f-91e717a0ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chrom_result(chrom, in_file, variants: list, verbose: bool=False):\n",
    "    # have to do pass to find all phenos to possible capture\n",
    "    chrom_qtl_df = read_csv(in_file, sep='\\s+', header=None)\n",
    "    chrom_qtl_df.columns = ['gene_info', 'variant_id', 'tss_distance', 'pval_nominal', 'slope']\n",
    "    if verbose:\n",
    "        print(f'read {chrom_qtl_df.shape}')\n",
    "    # split the gene into gene name and id\n",
    "    temp_df = chrom_qtl_df.gene_info.str.split('_', n=1, expand=True)\n",
    "    temp_df.columns = ['gene_name', 'gene_id']\n",
    "    chrom_qtl_df['gene_id'] = temp_df.gene_id\n",
    "    chrom_qtl_df['phenotype_id'] = temp_df.gene_name\n",
    "    if verbose:\n",
    "        print(f'after splitting gene info {chrom_qtl_df.shape}')    \n",
    "    out_columns = ['phenotype_id', 'variant_id', 'tss_distance', 'pval_nominal', 'slope']\n",
    "    chrom_qtl_df = chrom_qtl_df[out_columns]\n",
    "    # find traits tested against variants of interest with sufficicent p-value\n",
    "    oi_results = chrom_qtl_df.loc[(chrom_qtl_df['variant_id'].isin(variants)) & \n",
    "                                  (chrom_qtl_df['pval_nominal'] <= max_nominal)]\n",
    "    phenos_oi = list(oi_results['phenotype_id'].unique())\n",
    "    # do pass to keep results that belong those phenos\n",
    "    possible_results_oi = chrom_qtl_df.loc[chrom_qtl_df['phenotype_id'].isin(phenos_oi)].copy()\n",
    "    if verbose:\n",
    "        display(possible_results_oi.head())\n",
    "        print(phenos_oi)\n",
    "    return phenos_oi, possible_results_oi\n",
    "\n",
    "def process_qtl(trait_df: DataFrame, other_stats: DataFrame, num_samples: int) -> DataFrame:\n",
    "    \"\"\" Prep the QTL (or trait2) results for use in colocalization. \n",
    "        Where prep performs Wakefield Approx Bayes Factor, posterior probabliltiy,\n",
    "        and credible sets calculations and identification\n",
    "    Args:\n",
    "        trait_df (pandas.DataFrame) QTL results for a feature\n",
    "        other_stats (pandas.DataFrame) trait1 (or risk) stats to be used with these\n",
    "            qtl (or trait2) stats for colocalization\n",
    "        num_samples (int) number of samples used in for the qtl analysis, if set to 0 then\n",
    "            number of samples is present in results per variant\n",
    "    Returns:\n",
    "        (pandas.DataFrame) qtl results with ABF, PP, and credible sets computed\n",
    "    \"\"\"\n",
    "    # some feature QTL stats may also have multiple results per variants \n",
    "    # so need to reduce or remove these\n",
    "    # these are typically a results of variants that are multi-allelic like indels\n",
    "    trait_df = trait_df.drop_duplicates(subset=['variant_id'], keep='first')\n",
    "    # calculate the ABF's for the feature's QTL results\n",
    "    trait_df[\"logABF\"] = trait_df.apply(\n",
    "    lambda result: clc.calc_abf(pval=result.pval_nominal, maf=clc.freq_to_maf(result.allele_frequency),\n",
    "                                n=num_samples if num_samples != 0 else int(result.num_samples)), \n",
    "                                axis=1)    \n",
    "    trait_df = trait_df.sort_values(\"logABF\", ascending=False)\n",
    "    # calculate the posterior probability for each variant\n",
    "    trait_df['PP'] = clc.compute_pp(trait_df.logABF)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(trait_df)\n",
    "    # subset the feature QTL variants to just those present in the GWAS\n",
    "    trait_df = trait_df.loc[trait_df.variant_id.isin(other_stats.variant_id)] \n",
    "    return trait_df\n",
    "\n",
    "def process_gwas(trait_stats: DataFrame, other_stats: DataFrame) -> DataFrame:\n",
    "    \"\"\" Prep the risk (or trait1) results for use in colocalization. \n",
    "        Where prep performs subet of variant to those present in other (trait2/qtl),\n",
    "        Wakefield Approx Bayes Factor, posterior probabliltiy,\n",
    "        and credible sets calculations and identification\n",
    "    Args:\n",
    "        trait_stats (pandas.DataFrame) trait1 (or risk) stats to be used with these\n",
    "            qtl (or trait2) stats for colocalization\n",
    "        other_stats (pandas.DataFrame) trait2 (or qtl) stats to be used with these\n",
    "            risk (or trait2) stats for colocalization\n",
    "    Returns:\n",
    "        (pandas.DataFrame) risk results with ABF, PP, and credible sets computed\n",
    "    \"\"\" \n",
    "    # subset the risk summary stats by the feature's QTL variants present\n",
    "    ret_df = trait_stats.loc[trait_stats.variant_id.isin(other_stats.variant_id)].copy()\n",
    "    # calculate the ABF's for the risk results\n",
    "    ret_df['logABF'] = ret_df.apply(\n",
    "        lambda result: clc.calc_abf(pval=result.p_value, \n",
    "                                    maf=clc.freq_to_maf(result.effect_allele_frequency),\n",
    "                                    n=result.n_total, \n",
    "                                    prop_cases=result.case_prop), axis=1)\n",
    "    ret_df = ret_df.sort_values('logABF', ascending=False)  \n",
    "    # calculate the posterior probability for each variant\n",
    "    ret_df['PP'] = clc.compute_pp(ret_df.logABF)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(ret_df)\n",
    "    return ret_df\n",
    "\n",
    "def ensure_matched_indices(df1: DataFrame, df2: DataFrame) -> {DataFrame, DataFrame}:\n",
    "    \"\"\" make sure the two datasets are ordered the same\n",
    "        modifies both df1 and df2\n",
    "    Args:\n",
    "        df1 (pandas.DataFrame) risk or trait1 data\n",
    "        df2 (pandas.DataFrame) qtl or trait2 data\n",
    "    \"\"\" \n",
    "    # ensure that the risk and feature variants ABF's are ordered the same\n",
    "    df1.set_index('variant_id', inplace=True)\n",
    "    df2.set_index('variant_id', inplace=True)\n",
    "    shared_indices = df1.index.intersection(df2.index)\n",
    "    df1 = df1.loc[shared_indices,]\n",
    "    df2 = df2.loc[shared_indices,]\n",
    "    temp = df1.index.values == df2.index.values\n",
    "    return df1, df2\n",
    "\n",
    "def colocalize(t1_abfs, t2_abfs, feature: str) -> Series:\n",
    "    \"\"\" Perform the colocalization between trait1 and trait2 ABFs\n",
    "    Args:\n",
    "        t1_abfs (array_like) trait1's ABFs\n",
    "        t2_abfs (array_like) trait2's ABFs\n",
    "        feature (string) trait2's name or ID\n",
    "    Returns:\n",
    "        (pandas.Series) named colocalization posterior probabilities\n",
    "    \"\"\"\n",
    "    h_probs = clc.combine_abf(t1_abfs, t2_abfs)\n",
    "    names = [f'H{x}' for x in range(5)]\n",
    "    cl_result = Series(data=around(h_probs, decimals=3), index=names)\n",
    "    cl_result['feature'] = feature\n",
    "    return cl_result  \n",
    "\n",
    "def compute_combined_pp(t1_df: DataFrame, t2_df: DataFrame) -> DataFrame:\n",
    "    \"\"\" Compute the the combined ABFs posterior probabilities and credible sets\n",
    "    Args:\n",
    "        t1_df (pandas.DataFrame) risk or trait1's data\n",
    "        t2_df (pandas.DataFrame) qtl or trait2's data\n",
    "    Returns:\n",
    "        (pandas.DataFrame) t1_df combined with t2_df with PP and credible sets ID'd\n",
    "    \"\"\"\n",
    "    ret_df = merge(t1_df, t2_df, how='inner', on='variant_id', suffixes=('_risk', '_qtl'))\n",
    "    # calculate the posterior probability for each variant\n",
    "    ret_df['PP'] = clc.compute_pp(ret_df.logABF_risk + ret_df.logABF_qtl)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(ret_df)\n",
    "    ret_df.rename(columns={'PP': 'h4_pp'}, inplace=True)\n",
    "    return ret_df\n",
    "\n",
    "def process_cell_type(cell_type: str, cell_abbrv: str):\n",
    "    coloc_scores = []\n",
    "    coloc_h4_pps = None    \n",
    "    # process data by chromosome\n",
    "    for chrom in risk_chroms:\n",
    "        bryios_file = f'{public_dir}/bryois_brain_eqtl/{cell_type}.{chrom}.gz'\n",
    "        features_oi, results_to_test = load_chrom_result(chrom, bryios_file, index_variants)\n",
    "        if DEBUG:\n",
    "            print(f'chr{chrom}', end='.')            \n",
    "            print(features_oi)\n",
    "            print(results_to_test.shape)\n",
    "        # the allele freq's are not available in the fastq summary stats, \n",
    "        # here assume same population(s) are risk and use GWAS allele freqs\n",
    "        results_to_test = results_to_test.merge(gwas_stats_df.loc[gwas_stats_df.chromosome == chrom, \n",
    "                                                                  ['variant_id', 'effect_allele_frequency']], \n",
    "                                                how='left', on='variant_id')\n",
    "        # rename the allefe freq column\n",
    "        results_to_test.rename(columns={'effect_allele_frequency': 'allele_frequency'}, inplace=True)\n",
    "        # drop any with missing freq after merge\n",
    "        results_to_test = results_to_test.loc[~results_to_test.allele_frequency.isna()]\n",
    "        for feature in features_oi:\n",
    "            feature_df = results_to_test.loc[results_to_test.phenotype_id == feature]\n",
    "            feature_df = process_qtl(feature_df, gwas_stats_df, num_qtl_samples)\n",
    "            # prep the GWAS results\n",
    "            risk_df = process_gwas(gwas_stats_df, feature_df)\n",
    "            # ensure that the risk and feature variants ABF's are ordered the same\n",
    "            risk_df, feature_df = ensure_matched_indices(risk_df, feature_df)\n",
    "            # perform the colocalization\n",
    "            cl_result = colocalize(risk_df.logABF, feature_df.logABF, feature)\n",
    "            # if H4 is supported then compute H4.PP the H4 credible sets\n",
    "            cl_result['h4_supported'] = clc.h4_supported(cl_result)\n",
    "            if cl_result.H4 >= min_h4:\n",
    "                combined_df = compute_combined_pp(risk_df, feature_df)\n",
    "                coloc_h4_pps = concat([coloc_h4_pps, combined_df])                  \n",
    "            # add these scores to the rest\n",
    "            coloc_scores.append(cl_result)\n",
    "    # create a dataframe from the list of coloc scores    \n",
    "    coloc_scores_df = DataFrame(coloc_scores)\n",
    "    ### save the result files for this cell-type\n",
    "    set_name = set_name_frmt.format(cell_abbrv=cell_abbrv)\n",
    "    coloc_scores_files = f'{results_dir}/{set_name}_{dx}.coloc.pp.csv'\n",
    "    coloc_casuals_files = f'{results_dir}/{set_name}_{dx}.casuals.pp.parquet'\n",
    "    coloc_scores_df.to_csv(coloc_scores_files, index=False)\n",
    "    if not coloc_h4_pps is None:\n",
    "        coloc_h4_pps.to_parquet(coloc_casuals_files)\n",
    "    else:\n",
    "        print(f'{cell_type} no H4 supported so no coloc_h4_pps')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da59a4-7894-4815-a270-00acb4dbc68f",
   "metadata": {},
   "source": [
    "### load the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db625f4b-f5bf-4bc8-be25-159e68fcc22a",
   "metadata": {},
   "source": [
    "#### load the risk variants of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b94129-b8cd-4fa4-a074-878e0d7c3b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 1)\n",
      "CPU times: user 3.02 ms, sys: 0 ns, total: 3.02 ms\n",
      "Wall time: 2.62 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "variants_oi_df = read_csv(index_variants_file)\n",
    "print(variants_oi_df.shape)\n",
    "index_variants = list(variants_oi_df.variant.unique())\n",
    "if DEBUG:\n",
    "    display(variants_oi_df.head())\n",
    "    print(index_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d9e65-921e-478a-91b6-50fea44c2363",
   "metadata": {},
   "source": [
    "#### load the full gwas summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cfc367d-2255-4383-abfe-f3baff4afb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7769022, 12)\n",
      "CPU times: user 10.2 s, sys: 885 ms, total: 11.1 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gwas_stats_df = read_csv(gwas_sum_stats_file, sep='\\t')\n",
    "print(gwas_stats_df.shape)\n",
    "if DEBUG:\n",
    "    display(gwas_stats_df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b11de-af9e-423d-bd67-613300dce68d",
   "metadata": {},
   "source": [
    "#### set case proportion for GWAS summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308fb6ef-6753-4c40-b319-a2d84d25702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_stats_df['n_total'] = gwas_stats_df.n_cases + gwas_stats_df.n_controls\n",
    "    \n",
    "gwas_stats_df['case_prop'] = gwas_stats_df.n_cases / gwas_stats_df.n_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ffbf7-0d74-41bd-a646-a0a4d865d2b6",
   "metadata": {},
   "source": [
    "#### subset index variant stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cbf6db5-d58a-4b79-a2b4-e321a26ee1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 13)\n"
     ]
    }
   ],
   "source": [
    "index_stats_df = gwas_stats_df.loc[gwas_stats_df.variant_id.isin(index_variants)]\n",
    "print(index_stats_df.shape)\n",
    "if DEBUG:\n",
    "    display(index_stats_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18363b73-04a6-4818-8232-d4c15025155c",
   "metadata": {},
   "source": [
    "### load data, format data, and save new input files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d1bc6-4133-4eac-8f4b-8c533f9c1be9",
   "metadata": {},
   "source": [
    "#### Bryois et al result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2f92e-95e1-432f-ba2d-cd89581a5242",
   "metadata": {},
   "source": [
    "#### load and analyze the Bryois results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d553048-9718-4b2c-aaff-5ab3336af02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 1, 20, 21, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# get the list of chromsomes that have risk variant\n",
    "risk_chroms = list(index_stats_df.chromosome.unique())\n",
    "print(risk_chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc3b904-18ae-4efc-8273-f95f5578f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Astrocytes ####\n",
      "#### Endothelial.cells ####\n",
      "#### Excitatory.neurons ####\n",
      "#### Inhibitory.neurons ####\n",
      "#### Microglia ####\n",
      "#### OPCs...COPs ####\n",
      "#### Oligodendrocytes ####\n",
      "#### Pericytes ####\n",
      "CPU times: user 35min 13s, sys: 3min 10s, total: 38min 24s\n",
      "Wall time: 30min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "job_threads = []\n",
    "for cell_type, cell_abbrv in cell_names_dict.items():\n",
    "    print(f'#### {cell_type} ####')\n",
    "    this_thread = Thread(target=process_cell_type, args=(cell_type, cell_abbrv))\n",
    "    job_threads.append(this_thread)\n",
    "    this_thread.start()\n",
    "for job_thread in job_threads:\n",
    "        job_thread.join()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07921397-973a-4531-86fa-e2cba64b79ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 17 18:45:43 UTC 2023\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
