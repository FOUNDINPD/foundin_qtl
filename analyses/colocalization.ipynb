{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb22d5c-f18e-44d5-ba9b-056882d10428",
   "metadata": {},
   "source": [
    "## Notebook to run the colocalization analyses between cell-type differentiation based QTL and disease risk loci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2cb945-f92a-49d8-9af1-ff246b17a714",
   "metadata": {},
   "source": [
    "### From [coloc docs](https://chr1swallace.github.io/coloc/articles/a03_enumeration.html) for single causal variant assumption, posterior probabilities that the traits share their configurations\n",
    "𝐻0: neither trait has a genetic association in the region<br>\n",
    "𝐻1: only trait 1 has a genetic association in the region<br>\n",
    "𝐻2: only trait 2 has a genetic association in the region<br>\n",
    "𝐻3: both traits are associated, but with different causal variants<br>\n",
    "𝐻4: both traits are associated and share a single causal variant<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95245c93-9bb4-4ac7-a04c-bcea49ae7401",
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15889737-750b-4ae2-8b0b-7f373138aa47",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf49a28-0c00-4f61-92c2-ca20a3b42f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colocalization as clc\n",
    "from pandas import read_csv, DataFrame, read_parquet, Series, merge, concat\n",
    "from numpy import around"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa913dc-f7a1-4e80-99eb-994027b01506",
   "metadata": {},
   "source": [
    "#### set notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12400d4a-29f3-473c-9da3-b2bc3a80b0a8",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "day = ''\n",
    "modality = ''\n",
    "num_qtl_samples = 0\n",
    "file_type = '' # tensorqtl or metal+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6320dc1-00e0-4f96-9655-d2faf6e223a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming\n",
    "cohort = 'foundin'\n",
    "dx = 'PD'\n",
    "if file_type == 'tensorqtl':\n",
    "    set_name = f'{cohort}_{day}_{modality}'\n",
    "elif file_type == 'metal+':\n",
    "    set_name = f'{day}'\n",
    "\n",
    "# directories\n",
    "wrk_dir = '/labshare/raph/datasets/foundin_qtl'\n",
    "results_dir = f'{wrk_dir}/results'\n",
    "tensorqtl_dir = f'{wrk_dir}/tensorqtl'\n",
    "meta_dir = f'{wrk_dir}/meta'\n",
    "public_dir = f'{wrk_dir}/public'\n",
    "\n",
    "# input files\n",
    "shared_prelim_file = f'{results_dir}/{set_name}_{dx}.prelim_shared.cis.csv'\n",
    "# if agreement in place use summary stats that include 23andMe data\n",
    "gwas_sum_stats_file = f'{public_dir}/nalls_pd_gwas/pd_sumstats_23andme_buildGRCh38.tsv.gz'\n",
    "# gwas_sum_stats_file = f'{public_dir}/nalls_pd_gwas/pd_sumstats_no23andme_buildGRCh38.tsv.gz'\n",
    "index_variants_file = f'{public_dir}/nalls_pd_gwas/index_variants.list'    \n",
    "\n",
    "# output files\n",
    "coloc_scores_files = f'{results_dir}/{set_name}_{dx}.coloc.pp.csv'\n",
    "coloc_casuals_files = f'{results_dir}/{set_name}_{dx}.casuals.pp.parquet'\n",
    "\n",
    "# constant values\n",
    "DEBUG = False\n",
    "min_h4 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996dbd0-cc77-4e56-ad7c-0f6a58a501d5",
   "metadata": {},
   "source": [
    "#### utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209580c-a31a-4425-947c-d074ccf9cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qtl_results(chrom: str, in_dir: str, name: str, feature: str, \n",
    "                     verbose: bool=False) -> DataFrame:\n",
    "    \"\"\" Load the tensorQTL results for the feature specified from the \n",
    "        tensorQTL cis.map_nominal chromosome results parquet file\n",
    "    Args:\n",
    "        chrom (string) chromosome of feature and results\n",
    "        in_dir (string) directory name containing tensorQTL results\n",
    "        name (string) analysis set name (prefix) of tensorQTL results\n",
    "        feature (string) feature to load results for; ie gene, peak, etc\n",
    "        verbose (bool) show shape and head of loaded feature results, default=False\n",
    "    Returns:\n",
    "        (pandas.DataFrame) features cis.map_nominal tensorQTL results\n",
    "    \"\"\"\n",
    "    chrom_file = f'{in_dir}/{name}.cis_qtl_pairs.chr{chrom}.parquet'\n",
    "    chrom_qtl_df = read_parquet(chrom_file)\n",
    "    chrom_qtl_df = chrom_qtl_df.rename(columns={'phenotype_id': 'trait'})\n",
    "    feature_qtl_df = chrom_qtl_df.loc[chrom_qtl_df.trait == feature]\n",
    "    # make sure there aren't any bad values in the result\n",
    "    feature_qtl_df = feature_qtl_df.loc[~feature_qtl_df.pval_nominal.isna()]\n",
    "    if verbose:\n",
    "        print(f'{feature} has {feature_qtl_df.shape} results')\n",
    "        display(feature_qtl_df.head())\n",
    "    return feature_qtl_df\n",
    "    \n",
    "def process_qtl(trait_df: DataFrame, other_stats: DataFrame, num_samples: int) -> DataFrame:\n",
    "    \"\"\" Prep the QTL (or trait2) results for use in colocalization. \n",
    "        Where prep performs Wakefield Approx Bayes Factor, posterior probabliltiy,\n",
    "        and credible sets calculations and identification\n",
    "    Args:\n",
    "        trait_df (pandas.DataFrame) QTL results for a feature\n",
    "        other_stats (pandas.DataFrame) trait1 (or risk) stats to be used with these\n",
    "            qtl (or trait2) stats for colocalization\n",
    "        num_samples (int) number of samples used in for the qtl analysis, if set to 0 then\n",
    "            number of samples is present in results per variant\n",
    "    Returns:\n",
    "        (pandas.DataFrame) qtl results with ABF, PP, and credible sets computed\n",
    "    \"\"\"\n",
    "    # some feature QTL stats may also have multiple results per variants \n",
    "    # so need to reduce or remove these\n",
    "    # these are typically a results of variants that are multi-allelic like indels\n",
    "    trait_df = trait_df.drop_duplicates(subset=['variant_id'], keep='first').copy()\n",
    "    # calculate the ABF's for the feature's QTL results\n",
    "    trait_df['logABF'] = trait_df.apply(\n",
    "    lambda result: clc.calc_abf(pval=result.pvalue, maf=clc.freq_to_maf(result.af),\n",
    "                                n=num_samples if num_samples != 0 else int(result.num_samples)), \n",
    "                                axis=1)    \n",
    "    trait_df = trait_df.sort_values(\"logABF\", ascending=False)\n",
    "    # calculate the posterior probability for each variant\n",
    "    trait_df['PP'] = clc.compute_pp(trait_df.logABF)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(trait_df)\n",
    "    # subset the feature QTL variants to just those present in the GWAS\n",
    "    trait_df = trait_df.loc[trait_df.variant_id.isin(other_stats.variant_id)] \n",
    "    return trait_df\n",
    "\n",
    "def process_gwas(trait_stats: DataFrame, other_stats: DataFrame) -> DataFrame:\n",
    "    \"\"\" Prep the risk (or trait1) results for use in colocalization. \n",
    "        Where prep performs subet of variant to those present in other (trait2/qtl),\n",
    "        Wakefield Approx Bayes Factor, posterior probabliltiy,\n",
    "        and credible sets calculations and identification\n",
    "    Args:\n",
    "        trait_stats (pandas.DataFrame) trait1 (or risk) stats to be used with these\n",
    "            qtl (or trait2) stats for colocalization\n",
    "        other_stats (pandas.DataFrame) trait2 (or qtl) stats to be used with these\n",
    "            risk (or trait2) stats for colocalization\n",
    "    Returns:\n",
    "        (pandas.DataFrame) risk results with ABF, PP, and credible sets computed\n",
    "    \"\"\" \n",
    "    # subset the risk summary stats by the feature's QTL variants present\n",
    "    ret_df = trait_stats.loc[trait_stats.variant_id.isin(other_stats.variant_id)].copy()\n",
    "    # calculate the ABF's for the risk results\n",
    "    ret_df['logABF'] = ret_df.apply(\n",
    "        lambda result: clc.calc_abf(pval=result.p_value, \n",
    "                                    maf=clc.freq_to_maf(result.effect_allele_frequency),\n",
    "                                    n=result.n_total, \n",
    "                                    prop_cases=result.case_prop), axis=1)\n",
    "    ret_df = ret_df.sort_values('logABF', ascending=False)  \n",
    "    # calculate the posterior probability for each variant\n",
    "    ret_df['PP'] = clc.compute_pp(ret_df.logABF)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(ret_df)\n",
    "    return ret_df\n",
    "\n",
    "def ensure_matched_indices(df1: DataFrame, df2: DataFrame) -> {DataFrame, DataFrame}:\n",
    "    \"\"\" make sure the two datasets are ordered the same\n",
    "        modifies both df1 and df2\n",
    "    Args:\n",
    "        df1 (pandas.DataFrame) risk or trait1 data\n",
    "        df2 (pandas.DataFrame) qtl or trait2 data\n",
    "    \"\"\" \n",
    "    # ensure that the risk and feature variants ABF's are ordered the same\n",
    "    df1 = df1.set_index('variant_id')\n",
    "    df2 = df2.set_index('variant_id')\n",
    "    print('reindexing')\n",
    "    shared_indices = df1.index.intersection(df2.index)\n",
    "    df1 = df1.loc[shared_indices,]\n",
    "    df2 = df2.loc[shared_indices,]\n",
    "    temp = df1.index.values == df2.index.values\n",
    "    display(Series(temp).value_counts())\n",
    "    return df1, df2\n",
    "        \n",
    "def colocalize(t1_abfs, t2_abfs, feature: str) -> Series:\n",
    "    \"\"\" Perform the colocalization between trait1 and trait2 ABFs\n",
    "    Args:\n",
    "        t1_abfs (array_like) trait1's ABFs\n",
    "        t2_abfs (array_like) trait2's ABFs\n",
    "        feature (string) trait2's name or ID\n",
    "    Returns:\n",
    "        (pandas.Series) named colocalization posterior probabilities\n",
    "    \"\"\"\n",
    "    h_probs = clc.combine_abf(t1_abfs, t2_abfs)\n",
    "    names = [f'H{x}' for x in range(5)]\n",
    "    cl_result = Series(data=around(h_probs, decimals=3), index=names)\n",
    "    cl_result['feature'] = feature\n",
    "    return cl_result  \n",
    "\n",
    "def compute_combined_pp(t1_df: DataFrame, t2_df: DataFrame) -> DataFrame:\n",
    "    \"\"\" Compute the the combined ABFs posterior probabilities and credible sets\n",
    "    Args:\n",
    "        t1_df (pandas.DataFrame) risk or trait1's data\n",
    "        t2_df (pandas.DataFrame) qtl or trait2's data\n",
    "    Returns:\n",
    "        (pandas.DataFrame) t1_df combined with t2_df with PP and credible sets ID'd\n",
    "    \"\"\"\n",
    "    ret_df = merge(t1_df, t2_df, how='inner', on='variant_id', suffixes=('_risk', '_qtl'))\n",
    "    # calculate the posterior probability for each variant\n",
    "    ret_df['PP'] = clc.compute_pp(ret_df.logABF_risk + ret_df.logABF_qtl)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(ret_df)\n",
    "    ret_df = ret_df.rename(columns={'PP': 'h4_pp'})\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e77d9e-ba5a-4e2e-93b9-e9138eaed829",
   "metadata": {},
   "source": [
    "### load the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53210951-bb65-48e3-976f-2d5e1203cacd",
   "metadata": {},
   "source": [
    "#### load the preliminary check of signal sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757fd51-f1cc-4cab-af5e-8e6324a0a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "shared_prelim = read_csv(shared_prelim_file)\n",
    "print(f'shape of peliminary sharing results {shared_prelim.shape}')\n",
    "if file_type == 'tensorqtl':\n",
    "    shared_prelim = shared_prelim.rename(columns={'phenotype_id': 'trait'})\n",
    "print(f'number of unique features {shared_prelim.trait.nunique()}')\n",
    "print(f'features {shared_prelim.trait.unique()}')\n",
    "\n",
    "if DEBUG:\n",
    "    display(shared_prelim.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc12113-52d2-4940-9350-a1dedc5c5a69",
   "metadata": {
    "papermill": {
     "duration": 0.011883,
     "end_time": "2021-06-24T22:33:59.038114",
     "exception": false,
     "start_time": "2021-06-24T22:33:59.026231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### load the full gwas summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100b398-006f-4a8f-913c-375349cb6c26",
   "metadata": {
    "papermill": {
     "duration": 27.839467,
     "end_time": "2021-06-24T22:34:26.888216",
     "exception": false,
     "start_time": "2021-06-24T22:33:59.048749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gwas_stats_df = read_csv(gwas_sum_stats_file, sep='\\t')\n",
    "print(gwas_stats_df.shape)\n",
    "\n",
    "if DEBUG:\n",
    "    display(gwas_stats_df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49adbdc-6bb5-45a0-a851-42289e905ec7",
   "metadata": {},
   "source": [
    "#### some summary stats may have multiple results per variants so need to reduce or remove these\n",
    "these are typically a results of variants that are multi-allelic like indels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e10b66-2dc2-4dc7-ad11-80c483bc232f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gwas_stats_df = gwas_stats_df.drop_duplicates(subset=['variant_id'], keep='first')\n",
    "print(gwas_stats_df.shape)\n",
    "\n",
    "if DEBUG:\n",
    "    display(gwas_stats_df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303844b-79c1-4558-9c44-838cb226a8ad",
   "metadata": {},
   "source": [
    "#### set case proportion for GWAS summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ceaa2-f498-439a-b272-c53b69c1d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_stats_df['n_total'] = gwas_stats_df.n_cases + gwas_stats_df.n_controls\n",
    "    \n",
    "gwas_stats_df['case_prop'] = gwas_stats_df.n_cases / gwas_stats_df.n_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898a96f-a1d7-44b6-b828-20dc3b6e796f",
   "metadata": {},
   "source": [
    "### for each of the features that were detected in the risk and QTL sharing run the colocalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7091c42-4da4-4b07-8b72-fa6a835ba1c6",
   "metadata": {},
   "source": [
    "#### if using Metal+ meta-analysis results load full QTL now\n",
    "\n",
    "since the Metal meta results are in single parquet file load once instead of repeating in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edadae-3384-492a-9ebb-87d688ec5956",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if file_type == 'metal+':\n",
    "    qtl_full_file = f'{meta_dir}/{set_name}_meta_eqtl.parquet'\n",
    "    qtl_full_df = read_parquet(qtl_full_file)\n",
    "    # since annotated allele freqs from AMP-PD onto meta make sure no weird rare\n",
    "    qtl_full_df = qtl_full_df.loc[qtl_full_df.af >= 0.01]    \n",
    "    print(qtl_full_df.shape)\n",
    "    number_genes = qtl_full_df.trait.nunique()\n",
    "    print(f'full qtl for {number_genes} features')\n",
    "    if DEBUG:\n",
    "        display(qtl_full_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddacce0-e983-4619-8f58-513001c8e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "coloc_scores = []\n",
    "coloc_h4_pps = None\n",
    "for feature in shared_prelim.trait.unique():\n",
    "    print(feature, end=', ')\n",
    "    # prep the QTL results, qtl pulled differ by type\n",
    "    if file_type == 'tensorqtl':\n",
    "        # get the chromosome the feature is found on\n",
    "        chrom = shared_prelim.loc[shared_prelim.trait == feature]['chromosome'].unique()[0]\n",
    "        # load the feature's QTL results\n",
    "        feature_df = load_qtl_results(str(chrom), tensorqtl_dir, set_name, feature)\n",
    "        # make necessary keys consistent between file_types\n",
    "        feature_df = feature_df.rename(columns={'pval_nominal': 'pvalue'})\n",
    "        feature_df = process_qtl(feature_df, gwas_stats_df, num_qtl_samples)\n",
    "    elif file_type == 'metal+':\n",
    "        feature_df = qtl_full_df.loc[qtl_full_df.trait == feature].copy()\n",
    "        # make necessary keys consistent between file_types\n",
    "        feature_df = feature_df.rename(columns={'P-value': 'pvalue', 'Weight':'num_samples', \n",
    "                                                'variant': 'variant_id'})        \n",
    "        feature_df = process_qtl(feature_df, gwas_stats_df, num_samples=0)\n",
    "    # prep the GWAS results\n",
    "    risk_df = process_gwas(gwas_stats_df, feature_df)\n",
    "    # ensure that the risk and feature variants ABF's are ordered the same\n",
    "    risk_df, feature_df = ensure_matched_indices(risk_df, feature_df)\n",
    "    # perform the colocalization\n",
    "    cl_result = colocalize(risk_df.logABF, feature_df.logABF, feature)\n",
    "    # if H4 is supported then compute H4.PP the H4 credible sets\n",
    "    cl_result['h4_supported'] = clc.h4_supported(cl_result)\n",
    "    if cl_result.H4 > min_h4:\n",
    "        combined_df = compute_combined_pp(risk_df, feature_df)\n",
    "        coloc_h4_pps = concat([coloc_h4_pps, combined_df])        \n",
    "    # add these scores to the rest\n",
    "    coloc_scores.append(cl_result)\n",
    "# create a dataframe from the list of coloc scores    \n",
    "coloc_scores_df = DataFrame(coloc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe71ed-3932-4a13-a5cb-dfd1aa6a724e",
   "metadata": {},
   "source": [
    "### save the result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643f4f8-8356-43cc-b4e0-acdecec7521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coloc_scores_df.to_csv(coloc_scores_files, index=False)\n",
    "if not coloc_h4_pps is None:\n",
    "    coloc_h4_pps.to_parquet(coloc_casuals_files)\n",
    "else:\n",
    "    print('no H4 supported so no coloc_h4_pps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4083e-54a8-41d7-b2af-2291db80fc25",
   "metadata": {},
   "source": [
    "### which features appear to have colocalization support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435996f6-68bf-41a6-93f5-28c1f02a4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_max_rows = 20\n",
    "if coloc_scores_df.shape[0] <= view_max_rows:\n",
    "    # if 10 or less entries show them all\n",
    "    print('showing all results')\n",
    "    display(coloc_scores_df)\n",
    "else:\n",
    "    # get just the H4 supported results\n",
    "    temp = coloc_scores_df.loc[coloc_scores_df.h4_supported == True]\n",
    "    if temp.shape[0] <= view_max_rows:\n",
    "        # if 10 or less entries show them all\n",
    "        print('showing all H4 supported results')\n",
    "        display(temp)\n",
    "    else:\n",
    "        print('showing top H4 supported results')\n",
    "        display(temp.sort_values('H4', ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084bd4ac-7c32-4586-96b8-83f3ee5c9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not coloc_h4_pps is None:\n",
    "    display(coloc_h4_pps.groupby('trait').is95_credset.value_counts())\n",
    "    display(coloc_h4_pps.groupby('trait').is99_credset.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f55d8-f08c-40f6-9966-55196ad964cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
