{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0f7991-1142-46d8-bda9-2cd035499934",
   "metadata": {},
   "source": [
    "## Notebook for scan the public Jerber et al iPSC derived DA neruon cell-type eQTL results\n",
    "\n",
    "- [Jerber et. al.](https://pubmed.ncbi.nlm.nih.gov/33664506/)\n",
    "    - Jerber J, Seaton DD, Cuomo ASE et al. Population-scale single-cell RNA-seq profiling across dopaminergic neuron differentiation. Nat Genet 2021;53:304â€“12.\n",
    "    - 175 individuals for single-cell, 193 for psuedobulk\n",
    "    - using their day 52 untreated DA neuron results\n",
    "    \n",
    "![HipSci iPSC DAn cartoon](https://media.springernature.com/m312/springer-static/image/art%3A10.1038%2Fs41588-021-00801-6/MediaObjects/41588_2021_801_Fig1_HTML.png?as=webp) \n",
    "    \n",
    "Scan the Jerber results for any nominal intersect with PD risk and for these intersects then run colocalization, results have already have additional prep for use in the meta-eQTL analysis with FOUNDIN-PD iPSC derived DA neuron cell-type eQTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a353ef4e-7149-4d4d-89eb-b98cd11868f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 17:24:30 UTC 2023\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bac636-3f2b-456c-94a1-7de18dac947d",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a99407-d70e-4570-831f-1b2bb7adf751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, read_parquet, DataFrame, Series, merge, concat\n",
    "from threading import Thread\n",
    "from numpy import around\n",
    "import colocalization as clc\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53578c-5c3b-479b-ad98-348a38de95c9",
   "metadata": {},
   "source": [
    "#### set notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1f7f20-1ab4-4e9e-bbd1-2ec68b19a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming\n",
    "cohort = 'hipsci'\n",
    "day = 'D52'\n",
    "set_name_frmt = 'foundin_{day}_{cohort}-{cell_type}'\n",
    "dx = 'PD'\n",
    "\n",
    "# directories \n",
    "wrk_dir = '/labshare/raph/datasets/foundin_qtl'\n",
    "public_dir = f'{wrk_dir}/public'\n",
    "results_dir = f'{wrk_dir}/results'\n",
    "\n",
    "# input files\n",
    "# with agreement in place use full summary stats\n",
    "# gwas_sum_stats_file = f'{public_dir}/nalls_pd_gwas/pd_sumstats_no23andme_buildGRCh38.tsv.gz'\n",
    "gwas_sum_stats_file = f'{public_dir}/nalls_pd_gwas/pd_sumstats_23andme_buildGRCh38.tsv.gz'\n",
    "index_variants_file = f'{public_dir}/nalls_pd_gwas/index_variants.list'  \n",
    "\n",
    "# variables\n",
    "DEBUG = False\n",
    "max_nominal = 0.01\n",
    "cell_types = ['DA', 'pseudobulk']\n",
    "min_h4 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5415c7-ec76-4b37-8493-e5f1f913213b",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a55ed5-ef3c-4eef-9a7f-91e717a0ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eqtl_results(in_file: str, variants: list, verbose: bool=False):\n",
    "    # have to do pass to find all features to possible capture\n",
    "    qtl_df = read_parquet(in_file)\n",
    "    if verbose:\n",
    "        print(f'read {qtl_df.shape}')\n",
    "    qtl_df.rename(columns={'id': 'variant_id'}, inplace=True)\n",
    "    # find traits tested against variants of interest with sufficicent p-value\n",
    "    oi_results = qtl_df.loc[(qtl_df['variant_id'].isin(variants)) & \n",
    "                            (qtl_df['p_value'] <= max_nominal)]\n",
    "    phenos_oi = list(oi_results['feature_id'].unique())\n",
    "    # do pass to keep results that belong those phenos\n",
    "    possible_results_oi = qtl_df.loc[qtl_df['feature_id'].isin(phenos_oi)].copy()\n",
    "    if verbose:\n",
    "        display(possible_results_oi.head())\n",
    "        print(phenos_oi)\n",
    "    return phenos_oi, possible_results_oi\n",
    "\n",
    "def process_qtl(trait_df: DataFrame, other_stats: DataFrame) -> DataFrame:\n",
    "    \"\"\" Prep the QTL (or trait2) results for use in colocalization. \n",
    "        Where prep performs Wakefield Approx Bayes Factor, posterior probabliltiy,\n",
    "        and credible sets calculations and identification\n",
    "    Args:\n",
    "        trait_df (pandas.DataFrame) QTL results for a feature\n",
    "        other_stats (pandas.DataFrame) trait1 (or risk) stats to be used with these\n",
    "            qtl (or trait2) stats for colocalization\n",
    "    Returns:\n",
    "        (pandas.DataFrame) qtl results with ABF, PP, and credible sets computed\n",
    "    \"\"\"\n",
    "    # some feature QTL stats may also have multiple results per variants \n",
    "    # so need to reduce or remove these\n",
    "    # these are typically a results of variants that are multi-allelic like indels\n",
    "    trait_df = trait_df.drop_duplicates(subset=['variant_id'], keep='first')\n",
    "    # calculate the ABF's for the feature's QTL results\n",
    "    trait_df[\"logABF\"] = trait_df.apply(\n",
    "    lambda result: clc.calc_abf(pval=result.p_value, maf=clc.freq_to_maf(result.maf),\n",
    "                                n=result.n_samples), \n",
    "                                axis=1)    \n",
    "    trait_df = trait_df.sort_values(\"logABF\", ascending=False)\n",
    "    # calculate the posterior probability for each variant\n",
    "    trait_df['PP'] = clc.compute_pp(trait_df.logABF)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(trait_df)\n",
    "    # subset the feature QTL variants to just those present in the GWAS\n",
    "    trait_df = trait_df.loc[trait_df.variant_id.isin(other_stats.variant_id)] \n",
    "    return trait_df\n",
    "\n",
    "def process_gwas(trait_stats: DataFrame, other_stats: DataFrame) -> DataFrame:\n",
    "    \"\"\" Prep the risk (or trait1) results for use in colocalization. \n",
    "        Where prep performs subet of variant to those present in other (trait2/qtl),\n",
    "        Wakefield Approx Bayes Factor, posterior probabliltiy,\n",
    "        and credible sets calculations and identification\n",
    "    Args:\n",
    "        trait_stats (pandas.DataFrame) trait1 (or risk) stats to be used with these\n",
    "            qtl (or trait2) stats for colocalization\n",
    "        other_stats (pandas.DataFrame) trait2 (or qtl) stats to be used with these\n",
    "            risk (or trait2) stats for colocalization\n",
    "    Returns:\n",
    "        (pandas.DataFrame) risk results with ABF, PP, and credible sets computed\n",
    "    \"\"\" \n",
    "    # subset the risk summary stats by the feature's QTL variants present\n",
    "    ret_df = trait_stats.loc[trait_stats.variant_id.isin(other_stats.variant_id)].copy()\n",
    "    # calculate the ABF's for the risk results\n",
    "    ret_df['logABF'] = ret_df.apply(\n",
    "        lambda result: clc.calc_abf(pval=result.p_value, \n",
    "                                    maf=clc.freq_to_maf(result.effect_allele_frequency),\n",
    "                                    n=result.n_total, \n",
    "                                    prop_cases=result.case_prop), axis=1)\n",
    "    ret_df = ret_df.sort_values('logABF', ascending=False)  \n",
    "    # calculate the posterior probability for each variant\n",
    "    ret_df['PP'] = clc.compute_pp(ret_df.logABF)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(ret_df)\n",
    "    return ret_df\n",
    "\n",
    "def ensure_matched_indices(df1: DataFrame, df2: DataFrame) -> {DataFrame, DataFrame}:\n",
    "    \"\"\" make sure the two datasets are ordered the same\n",
    "        modifies both df1 and df2\n",
    "    Args:\n",
    "        df1 (pandas.DataFrame) risk or trait1 data\n",
    "        df2 (pandas.DataFrame) qtl or trait2 data\n",
    "    \"\"\" \n",
    "    # ensure that the risk and feature variants ABF's are ordered the same\n",
    "    df1.set_index('variant_id', inplace=True)\n",
    "    df2.set_index('variant_id', inplace=True)\n",
    "    shared_indices = df1.index.intersection(df2.index)\n",
    "    df1 = df1.loc[shared_indices,]\n",
    "    df2 = df2.loc[shared_indices,]\n",
    "    temp = df1.index.values == df2.index.values\n",
    "    return df1, df2\n",
    "\n",
    "def colocalize(t1_abfs, t2_abfs, feature: str) -> Series:\n",
    "    \"\"\" Perform the colocalization between trait1 and trait2 ABFs\n",
    "    Args:\n",
    "        t1_abfs (array_like) trait1's ABFs\n",
    "        t2_abfs (array_like) trait2's ABFs\n",
    "        feature (string) trait2's name or ID\n",
    "    Returns:\n",
    "        (pandas.Series) named colocalization posterior probabilities\n",
    "    \"\"\"\n",
    "    h_probs = clc.combine_abf(t1_abfs, t2_abfs)\n",
    "    names = [f'H{x}' for x in range(5)]\n",
    "    cl_result = Series(data=around(h_probs, decimals=3), index=names)\n",
    "    cl_result['feature'] = feature\n",
    "    return cl_result  \n",
    "\n",
    "def compute_combined_pp(t1_df: DataFrame, t2_df: DataFrame) -> DataFrame:\n",
    "    \"\"\" Compute the the combined ABFs posterior probabilities and credible sets\n",
    "    Args:\n",
    "        t1_df (pandas.DataFrame) risk or trait1's data\n",
    "        t2_df (pandas.DataFrame) qtl or trait2's data\n",
    "    Returns:\n",
    "        (pandas.DataFrame) t1_df combined with t2_df with PP and credible sets ID'd\n",
    "    \"\"\"\n",
    "    ret_df = merge(t1_df, t2_df, how='inner', on='variant_id', suffixes=('_risk', '_qtl'))\n",
    "    # calculate the posterior probability for each variant\n",
    "    ret_df['PP'] = clc.compute_pp(ret_df.logABF_risk + ret_df.logABF_qtl)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(ret_df)\n",
    "    ret_df.rename(columns={'PP': 'h4_pp'}, inplace=True)\n",
    "    return ret_df\n",
    "\n",
    "def process_results(cell_type: str, verbose: bool=False):\n",
    "    coloc_scores = []\n",
    "    coloc_h4_pps = None \n",
    "    this_file = f'{public_dir}/jerber_da_neuron_eqtl/{cohort}_{day}_{cell_type}.untreated.qtl_results_all.hg38.parquet'\n",
    "    features_oi, results_to_test = load_eqtl_results(this_file, index_variants)\n",
    "    if verbose:\n",
    "        print(features_oi)\n",
    "        print(results_to_test.shape)\n",
    "    for feature in features_oi:\n",
    "        feature_df = results_to_test.loc[results_to_test.feature_id == feature]\n",
    "        feature_df = process_qtl(feature_df, gwas_stats_df)\n",
    "        # prep the GWAS results\n",
    "        risk_df = process_gwas(gwas_stats_df, feature_df)\n",
    "        # ensure that the risk and feature variants ABF's are ordered the same\n",
    "        risk_df, feature_df = ensure_matched_indices(risk_df, feature_df)\n",
    "        # perform the colocalization\n",
    "        cl_result = colocalize(risk_df.logABF, feature_df.logABF, feature)\n",
    "        # if H4 is supported then compute H4.PP the H4 credible sets\n",
    "        cl_result['h4_supported'] = clc.h4_supported(cl_result)\n",
    "        if cl_result.H4 >= min_h4:\n",
    "            combined_df = compute_combined_pp(risk_df, feature_df)\n",
    "            coloc_h4_pps = concat([coloc_h4_pps, combined_df])                  \n",
    "        # add these scores to the rest\n",
    "        coloc_scores.append(cl_result)\n",
    "    # create a dataframe from the list of coloc scores    \n",
    "    coloc_scores_df = DataFrame(coloc_scores)\n",
    "    ### save the result files for this cell-type\n",
    "    set_name = set_name_frmt.format(day=day, cohort=cohort, cell_type=cell_type)    \n",
    "    coloc_scores_files = f'{results_dir}/{set_name}_{dx}.coloc.pp.csv'\n",
    "    coloc_casuals_files = f'{results_dir}/{set_name}_{dx}.casuals.pp.parquet'\n",
    "    if verbose:\n",
    "        print(set_name, coloc_scores_files)\n",
    "    coloc_scores_df.to_csv(coloc_scores_files, index=False)\n",
    "    if not coloc_h4_pps is None:\n",
    "        coloc_h4_pps.to_parquet(coloc_casuals_files)\n",
    "    else:\n",
    "        print(f'{cell_type} no H4 supported so no coloc_h4_pps')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da59a4-7894-4815-a270-00acb4dbc68f",
   "metadata": {},
   "source": [
    "### load the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db625f4b-f5bf-4bc8-be25-159e68fcc22a",
   "metadata": {},
   "source": [
    "#### load the risk variants of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b94129-b8cd-4fa4-a074-878e0d7c3b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 1)\n",
      "CPU times: user 1.32 ms, sys: 2.25 ms, total: 3.57 ms\n",
      "Wall time: 3.15 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "variants_oi_df = read_csv(index_variants_file)\n",
    "print(variants_oi_df.shape)\n",
    "index_variants = list(variants_oi_df.variant.unique())\n",
    "if DEBUG:\n",
    "    display(variants_oi_df.head())\n",
    "    print(index_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d9e65-921e-478a-91b6-50fea44c2363",
   "metadata": {},
   "source": [
    "#### load the full gwas summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cfc367d-2255-4383-abfe-f3baff4afb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7769022, 12)\n",
      "CPU times: user 9.84 s, sys: 913 ms, total: 10.8 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gwas_stats_df = read_csv(gwas_sum_stats_file, sep='\\t')\n",
    "print(gwas_stats_df.shape)\n",
    "if DEBUG:\n",
    "    display(gwas_stats_df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b11de-af9e-423d-bd67-613300dce68d",
   "metadata": {},
   "source": [
    "#### set case proportion for GWAS summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308fb6ef-6753-4c40-b319-a2d84d25702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_stats_df['n_total'] = gwas_stats_df.n_cases + gwas_stats_df.n_controls\n",
    "    \n",
    "gwas_stats_df['case_prop'] = gwas_stats_df.n_cases / gwas_stats_df.n_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ffbf7-0d74-41bd-a646-a0a4d865d2b6",
   "metadata": {},
   "source": [
    "#### subset index variant stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cbf6db5-d58a-4b79-a2b4-e321a26ee1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 13)\n"
     ]
    }
   ],
   "source": [
    "index_stats_df = gwas_stats_df.loc[gwas_stats_df.variant_id.isin(index_variants)]\n",
    "print(index_stats_df.shape)\n",
    "if DEBUG:\n",
    "    display(index_stats_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18363b73-04a6-4818-8232-d4c15025155c",
   "metadata": {},
   "source": [
    "### load data, format data, and save new input files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d1bc6-4133-4eac-8f4b-8c533f9c1be9",
   "metadata": {},
   "source": [
    "#### Jerber et al result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2f92e-95e1-432f-ba2d-cd89581a5242",
   "metadata": {},
   "source": [
    "#### load and analyze the Jerber results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfc3b904-18ae-4efc-8273-f95f5578f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### DA ####\n",
      "#### pseudobulk ####\n",
      "a cell-type finished\n",
      "a cell-type finished\n",
      "CPU times: user 53.8 ms, sys: 530 ms, total: 584 ms\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fs_list = []\n",
    "with concurrent.futures.ProcessPoolExecutor() as ppe:\n",
    "    for cell_type in cell_types:\n",
    "        print(f'#### {cell_type} ####')\n",
    "        fs_list.append(ppe.submit(process_results, cell_type, DEBUG))\n",
    "for future in concurrent.futures.as_completed(fs_list):\n",
    "    print('a cell-type finished')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07921397-973a-4531-86fa-e2cba64b79ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 17:27:58 UTC 2023\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
