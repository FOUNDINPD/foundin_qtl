{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0f7991-1142-46d8-bda9-2cd035499934",
   "metadata": {},
   "source": [
    "## Notebook for scan the public MetaBrain meta-eQTL analysis for multiple brain regions\n",
    "\n",
    "- [de Klein et. al.](https://pubmed.ncbi.nlm.nih.gov/36823318/)\n",
    "    - de Klein N, Tsai EA, Vochteloo M et al. Brain expression quantitative trait locus and network analyses reveal downstream effects and putative drivers for brain-related diseases. Nat Genet 2023;55:377â€“88.\n",
    "    - [Meta-analysis of 8,613 samples from 14 brain datasets](https://www.metabrain.nl/)\n",
    "    ![MetaBrain cartoon](https://www.metabrain.nl/img/metabrain_study_overview_v2.png)    \n",
    "    - since the PD risk is based on Euro ancestry GWAS, here only considering the Euro ancestry QTL data\n",
    "    \n",
    "Scan the MetaBrain results for any nominal intersect with PD risk and for these intersects then run colocalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353ef4e-7149-4d4d-89eb-b98cd11868f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bac636-3f2b-456c-94a1-7de18dac947d",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a99407-d70e-4570-831f-1b2bb7adf751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame, Series, merge, concat\n",
    "from numpy import around\n",
    "import colocalization as clc\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53578c-5c3b-479b-ad98-348a38de95c9",
   "metadata": {},
   "source": [
    "#### set notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f7f20-1ab4-4e9e-bbd1-2ec68b19a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming\n",
    "set_name_frmt = 'foundin_daNA_MetaBrain-{region_abbrv}'\n",
    "dx = 'PD'\n",
    "\n",
    "# directories \n",
    "wrk_dir = '/labshare/raph/datasets/foundin_qtl'\n",
    "public_dir = f'{wrk_dir}/public'\n",
    "results_dir = f'{wrk_dir}/results'\n",
    "\n",
    "# input files\n",
    "# with agreement in place use full summary stats\n",
    "# gwas_sum_stats_file = f'{public_dir}/nalls_pd_gwas/pd_sumstats_no23andme_buildGRCh38.tsv.gz'\n",
    "gwas_sum_stats_file = f'{public_dir}/nalls_pd_gwas/pd_sumstats_23andme_buildGRCh38.tsv.gz'\n",
    "index_variants_file = f'{public_dir}/nalls_pd_gwas/index_variants.list'  \n",
    "\n",
    "# variables\n",
    "DEBUG = False\n",
    "max_nominal = 0.01\n",
    "region_names_dict = {\n",
    "    'basalganglia-EUR-30PCs': 'basalganglia', \n",
    "    'cerebellum-EUR-60PCs': 'cerebellum', \n",
    "    'cortex-EUR-80PCs': 'cortex', 'hippocampus-EUR-30PCs': 'hippocampus', \n",
    "    'spinalcord-EUR-20PCs': 'spinalcord'\n",
    "}\n",
    "min_h4 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5415c7-ec76-4b37-8493-e5f1f913213b",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a55ed5-ef3c-4eef-9a7f-91e717a0ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chrom_result(chrom, in_file, variants: list, verbose: bool=False):\n",
    "    # have to do pass to find all phenos to possible capture\n",
    "    chrom_qtl_df = read_csv(in_file, sep='\\s+')\n",
    "    if verbose:\n",
    "        print(f'read {chrom_qtl_df.shape}')\n",
    "    # split the SNP col to get variant dbSNP ID\n",
    "    temp_df = chrom_qtl_df.SNP.str.split(':', expand=True)\n",
    "    temp_df.columns = ['chrom', 'pos', 'variant_id', 'alleles']\n",
    "    chrom_qtl_df['variant_id'] = temp_df.variant_id\n",
    "    if verbose:\n",
    "        print(f'after splitting variant info {chrom_qtl_df.shape}')    \n",
    "    # find traits tested against variants of interest with sufficicent p-value\n",
    "    oi_results = chrom_qtl_df.loc[(chrom_qtl_df['variant_id'].isin(variants)) & \n",
    "                                  (chrom_qtl_df['MetaP'] <= max_nominal)]\n",
    "    phenos_oi = list(oi_results['GeneSymbol'].unique())\n",
    "    # do pass to keep results that belong those phenos\n",
    "    possible_results_oi = chrom_qtl_df.loc[chrom_qtl_df['GeneSymbol'].isin(phenos_oi)].copy()\n",
    "    if verbose:\n",
    "        display(possible_results_oi.head())\n",
    "        print(phenos_oi)\n",
    "    return phenos_oi, possible_results_oi\n",
    "\n",
    "def process_qtl(trait_df: DataFrame, other_stats: DataFrame) -> DataFrame:\n",
    "    \"\"\" Prep the QTL (or trait2) results for use in colocalization. \n",
    "        Where prep performs Wakefield Approx Bayes Factor, posterior probabliltiy,\n",
    "        and credible sets calculations and identification\n",
    "    Args:\n",
    "        trait_df (pandas.DataFrame) QTL results for a feature\n",
    "        other_stats (pandas.DataFrame) trait1 (or risk) stats to be used with these\n",
    "            qtl (or trait2) stats for colocalization\n",
    "        num_samples (int) number of samples used in for the qtl analysis, if set to 0 then\n",
    "            number of samples is present in results per variant\n",
    "    Returns:\n",
    "        (pandas.DataFrame) qtl results with ABF, PP, and credible sets computed\n",
    "    \"\"\"\n",
    "    # some feature QTL stats may also have multiple results per variants \n",
    "    # so need to reduce or remove these\n",
    "    # these are typically a results of variants that are multi-allelic like indels\n",
    "    trait_df = trait_df.drop_duplicates(subset=['variant_id'], keep='first')\n",
    "    # calculate the ABF's for the feature's QTL results\n",
    "    trait_df[\"logABF\"] = trait_df.apply(\n",
    "    lambda result: clc.calc_abf(pval=result.MetaP, maf=clc.freq_to_maf(result.SNPEffectAlleleFreq),\n",
    "                                n=result.MetaPN), axis=1)    \n",
    "    trait_df = trait_df.sort_values(\"logABF\", ascending=False)\n",
    "    # calculate the posterior probability for each variant\n",
    "    trait_df['PP'] = clc.compute_pp(trait_df.logABF)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(trait_df)\n",
    "    # subset the feature QTL variants to just those present in the GWAS\n",
    "    trait_df = trait_df.loc[trait_df.variant_id.isin(other_stats.variant_id)] \n",
    "    return trait_df\n",
    "\n",
    "def process_gwas(trait_stats: DataFrame, other_stats: DataFrame) -> DataFrame:\n",
    "    \"\"\" Prep the risk (or trait1) results for use in colocalization. \n",
    "        Where prep performs subet of variant to those present in other (trait2/qtl),\n",
    "        Wakefield Approx Bayes Factor, posterior probabliltiy,\n",
    "        and credible sets calculations and identification\n",
    "    Args:\n",
    "        trait_stats (pandas.DataFrame) trait1 (or risk) stats to be used with these\n",
    "            qtl (or trait2) stats for colocalization\n",
    "        other_stats (pandas.DataFrame) trait2 (or qtl) stats to be used with these\n",
    "            risk (or trait2) stats for colocalization\n",
    "    Returns:\n",
    "        (pandas.DataFrame) risk results with ABF, PP, and credible sets computed\n",
    "    \"\"\" \n",
    "    # subset the risk summary stats by the feature's QTL variants present\n",
    "    ret_df = trait_stats.loc[trait_stats.variant_id.isin(other_stats.variant_id)].copy()\n",
    "    # calculate the ABF's for the risk results\n",
    "    ret_df['logABF'] = ret_df.apply(\n",
    "        lambda result: clc.calc_abf(pval=result.p_value, \n",
    "                                    maf=clc.freq_to_maf(result.effect_allele_frequency),\n",
    "                                    n=result.n_total, \n",
    "                                    prop_cases=result.case_prop), axis=1)\n",
    "    ret_df = ret_df.sort_values('logABF', ascending=False)  \n",
    "    # calculate the posterior probability for each variant\n",
    "    ret_df['PP'] = clc.compute_pp(ret_df.logABF)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(ret_df)\n",
    "    return ret_df\n",
    "\n",
    "def ensure_matched_indices(df1: DataFrame, df2: DataFrame) -> {DataFrame, DataFrame}:\n",
    "    \"\"\" make sure the two datasets are ordered the same\n",
    "        modifies both df1 and df2\n",
    "    Args:\n",
    "        df1 (pandas.DataFrame) risk or trait1 data\n",
    "        df2 (pandas.DataFrame) qtl or trait2 data\n",
    "    \"\"\" \n",
    "    # ensure that the risk and feature variants ABF's are ordered the same\n",
    "    df1.set_index('variant_id', inplace=True)\n",
    "    df2.set_index('variant_id', inplace=True)\n",
    "    shared_indices = df1.index.intersection(df2.index)\n",
    "    df1 = df1.loc[shared_indices,]\n",
    "    df2 = df2.loc[shared_indices,]\n",
    "    temp = df1.index.values == df2.index.values\n",
    "    return df1, df2\n",
    "\n",
    "def colocalize(t1_abfs, t2_abfs, feature: str) -> Series:\n",
    "    \"\"\" Perform the colocalization between trait1 and trait2 ABFs\n",
    "    Args:\n",
    "        t1_abfs (array_like) trait1's ABFs\n",
    "        t2_abfs (array_like) trait2's ABFs\n",
    "        feature (string) trait2's name or ID\n",
    "    Returns:\n",
    "        (pandas.Series) named colocalization posterior probabilities\n",
    "    \"\"\"\n",
    "    h_probs = clc.combine_abf(t1_abfs, t2_abfs)\n",
    "    names = [f'H{x}' for x in range(5)]\n",
    "    cl_result = Series(data=around(h_probs, decimals=3), index=names)\n",
    "    cl_result['feature'] = feature\n",
    "    return cl_result  \n",
    "\n",
    "def compute_combined_pp(t1_df: DataFrame, t2_df: DataFrame) -> DataFrame:\n",
    "    \"\"\" Compute the the combined ABFs posterior probabilities and credible sets\n",
    "    Args:\n",
    "        t1_df (pandas.DataFrame) risk or trait1's data\n",
    "        t2_df (pandas.DataFrame) qtl or trait2's data\n",
    "    Returns:\n",
    "        (pandas.DataFrame) t1_df combined with t2_df with PP and credible sets ID'd\n",
    "    \"\"\"\n",
    "    ret_df = merge(t1_df, t2_df, how='inner', on='variant_id', suffixes=('_risk', '_qtl'))\n",
    "    # calculate the posterior probability for each variant\n",
    "    ret_df['PP'] = clc.compute_pp(ret_df.logABF_risk + ret_df.logABF_qtl)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(ret_df)\n",
    "    ret_df.rename(columns={'PP': 'h4_pp'}, inplace=True)\n",
    "    return ret_df\n",
    "\n",
    "def process_region(region: str, region_abbrv: str):\n",
    "    coloc_scores = []\n",
    "    coloc_h4_pps = None    \n",
    "    # process data by chromosome\n",
    "    for chrom in risk_chroms:\n",
    "        metabrain_file = f'{public_dir}/metabrain/2021-07-23-{region}-chr{chrom}.txt.gz'\n",
    "        features_oi, results_to_test = load_chrom_result(chrom, metabrain_file, index_variants)\n",
    "        if DEBUG:\n",
    "            print(f'chr{chrom}', end='.')            \n",
    "            print(features_oi)\n",
    "            print(results_to_test.shape)\n",
    "        for feature in features_oi:\n",
    "            feature_df = results_to_test.loc[results_to_test.GeneSymbol == feature]\n",
    "            feature_df = process_qtl(feature_df, gwas_stats_df)\n",
    "            # prep the GWAS results\n",
    "            risk_df = process_gwas(gwas_stats_df, feature_df)\n",
    "            # ensure that the risk and feature variants ABF's are ordered the same\n",
    "            risk_df, feature_df = ensure_matched_indices(risk_df, feature_df)\n",
    "            # perform the colocalization\n",
    "            cl_result = colocalize(risk_df.logABF, feature_df.logABF, feature)\n",
    "            # if H4 is supported then compute H4.PP the H4 credible sets\n",
    "            cl_result['h4_supported'] = clc.h4_supported(cl_result)\n",
    "            if cl_result.H4 > min_h4:\n",
    "                combined_df = compute_combined_pp(risk_df, feature_df)\n",
    "                coloc_h4_pps = concat([coloc_h4_pps, combined_df])                  \n",
    "            # add these scores to the rest\n",
    "            coloc_scores.append(cl_result)\n",
    "    # create a dataframe from the list of coloc scores    \n",
    "    coloc_scores_df = DataFrame(coloc_scores)\n",
    "    ### save the result files for this cell-type\n",
    "    set_name = set_name_frmt.format(region_abbrv=region_abbrv)\n",
    "    coloc_scores_files = f'{results_dir}/{set_name}_{dx}.coloc.pp.csv'\n",
    "    coloc_casuals_files = f'{results_dir}/{set_name}_{dx}.casuals.pp.parquet'\n",
    "    coloc_scores_df.to_csv(coloc_scores_files, index=False)\n",
    "    if not coloc_h4_pps is None:\n",
    "        coloc_h4_pps.to_parquet(coloc_casuals_files)\n",
    "    else:\n",
    "        print(f'{region} no H4 supported so no coloc_h4_pps')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da59a4-7894-4815-a270-00acb4dbc68f",
   "metadata": {},
   "source": [
    "### load the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db625f4b-f5bf-4bc8-be25-159e68fcc22a",
   "metadata": {},
   "source": [
    "#### load the risk variants of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b94129-b8cd-4fa4-a074-878e0d7c3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "variants_oi_df = read_csv(index_variants_file)\n",
    "print(variants_oi_df.shape)\n",
    "index_variants = list(variants_oi_df.variant.unique())\n",
    "if DEBUG:\n",
    "    display(variants_oi_df.head())\n",
    "    print(index_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d9e65-921e-478a-91b6-50fea44c2363",
   "metadata": {},
   "source": [
    "#### load the full gwas summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc367d-2255-4383-abfe-f3baff4afb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gwas_stats_df = read_csv(gwas_sum_stats_file, sep='\\t')\n",
    "print(gwas_stats_df.shape)\n",
    "if DEBUG:\n",
    "    display(gwas_stats_df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b11de-af9e-423d-bd67-613300dce68d",
   "metadata": {},
   "source": [
    "#### set case proportion for GWAS summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308fb6ef-6753-4c40-b319-a2d84d25702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_stats_df['n_total'] = gwas_stats_df.n_cases + gwas_stats_df.n_controls\n",
    "    \n",
    "gwas_stats_df['case_prop'] = gwas_stats_df.n_cases / gwas_stats_df.n_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ffbf7-0d74-41bd-a646-a0a4d865d2b6",
   "metadata": {},
   "source": [
    "#### subset index variant stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf6db5-d58a-4b79-a2b4-e321a26ee1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_stats_df = gwas_stats_df.loc[gwas_stats_df.variant_id.isin(index_variants)]\n",
    "print(index_stats_df.shape)\n",
    "if DEBUG:\n",
    "    display(index_stats_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18363b73-04a6-4818-8232-d4c15025155c",
   "metadata": {},
   "source": [
    "### load data, format data, and save new input files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2f92e-95e1-432f-ba2d-cd89581a5242",
   "metadata": {},
   "source": [
    "#### load and analyze the MetaBrain results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d553048-9718-4b2c-aaff-5ab3336af02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of chromsomes that have risk variant\n",
    "risk_chroms = list(index_stats_df.chromosome.unique())\n",
    "print(risk_chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e6c39-3d8a-4f8e-9244-b27c8af77910",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fs_list = []\n",
    "with concurrent.futures.ProcessPoolExecutor() as ppe:\n",
    "    for region, region_abbrv in region_names_dict.items():\n",
    "        print(f'#### {region} ####')\n",
    "        fs_list.append(ppe.submit(process_region, region, region_abbrv))\n",
    "for future in concurrent.futures.as_completed(fs_list):\n",
    "    print('a region finished')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07921397-973a-4531-86fa-e2cba64b79ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
