{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0f7991-1142-46d8-bda9-2cd035499934",
   "metadata": {},
   "source": [
    "## Notebook for scan the meta-analysis eQTL results for PD risk and colocalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a353ef4e-7149-4d4d-89eb-b98cd11868f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 23 17:14:19 EDT 2024\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bac636-3f2b-456c-94a1-7de18dac947d",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a99407-d70e-4570-831f-1b2bb7adf751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame, Series, merge, concat, read_parquet\n",
    "from numpy import around\n",
    "import colocalization as clc\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53578c-5c3b-479b-ad98-348a38de95c9",
   "metadata": {},
   "source": [
    "#### set notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1f7f20-1ab4-4e9e-bbd1-2ec68b19a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming\n",
    "set_name_frmt = 'foundin_daNA_{modality}'\n",
    "dx = 'PD'\n",
    "\n",
    "# directories \n",
    "wrk_dir = '/labshare/raph/datasets/foundin_qtl'\n",
    "public_dir = f'{wrk_dir}/public'\n",
    "meta_dir = f'{wrk_dir}/meta'\n",
    "results_dir = f'{wrk_dir}/results'\n",
    "\n",
    "# input files\n",
    "# with agreement in place use full summary stats\n",
    "# gwas_sum_stats_file = f'{public_dir}/nalls_pd_gwas/pd_sumstats_no23andme_buildGRCh38.tsv.gz'\n",
    "gwas_sum_stats_file = f'{public_dir}/nalls_pd_gwas/pd_sumstats_23andme_buildGRCh38.tsv.gz'\n",
    "index_variants_file = f'{public_dir}/nalls_pd_gwas/index_variants.list'  \n",
    "\n",
    "# variables\n",
    "DEBUG = False\n",
    "max_nominal = 0.01\n",
    "alpha = 0.05\n",
    "modalities = ['Bulk-meta', 'DAn-meta']\n",
    "min_h4 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5415c7-ec76-4b37-8493-e5f1f913213b",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a55ed5-ef3c-4eef-9a7f-91e717a0ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta_eqtl_results(in_dir: str, name: str) -> DataFrame:\n",
    "    qtl_full_file = f'{in_dir}/{name}_metal_eqtl.parquet'\n",
    "    qtl_full_df = read_parquet(qtl_full_file)\n",
    "    print(f'shape of {name} results {qtl_full_df.shape}')\n",
    "    # rename variant col so consistent with gwas\n",
    "    qtl_full_df = qtl_full_df.rename(columns={'variant': 'variant_id', 'P-value': 'p_value'})\n",
    "    # load the qtl tops and only keep full results for QTL detected by FDR\n",
    "    top_file = f'{in_dir}/{name}_metal_eqtl_top.csv'\n",
    "    tops_df = read_csv(top_file)\n",
    "    tops_df = tops_df.loc[tops_df.bh_fdr <= alpha]\n",
    "    qtl_full_df = qtl_full_df.loc[qtl_full_df.trait.isin(tops_df.trait)]\n",
    "    number_genes = qtl_full_df.trait.nunique()\n",
    "    print(f'full qtl for {number_genes} features for {name}')\n",
    "    if DEBUG:\n",
    "        display(qtl_full_df.head())\n",
    "    return qtl_full_df\n",
    "\n",
    "def find_intersecing_traits(df: DataFrame, variants: list, name: str) -> list:\n",
    "    # find traits tested against variants of interest with sufficicent p-value\n",
    "    oi_results = df.loc[(df.variant_id.isin(variants)) & \n",
    "                        (df.p_value <= max_nominal)]\n",
    "    print(f'{name} intersect shape {oi_results.shape}')\n",
    "    traits_oi = list(oi_results.trait.unique())\n",
    "    print(f'{name} number of traits {len(traits_oi)}')\n",
    "    if DEBUG:\n",
    "        print(name, traits_oi)\n",
    "    return traits_oi\n",
    "\n",
    "def process_qtl(trait_df: DataFrame, other_stats: DataFrame) -> DataFrame:\n",
    "    \"\"\" Prep the QTL (or trait2) results for use in colocalization. \n",
    "        Where prep performs Wakefield Approx Bayes Factor, posterior probabliltiy,\n",
    "        and credible sets calculations and identification\n",
    "    Args:\n",
    "        trait_df (pandas.DataFrame) QTL results for a feature\n",
    "        other_stats (pandas.DataFrame) trait1 (or risk) stats to be used with these\n",
    "            qtl (or trait2) stats for colocalization\n",
    "    Returns:\n",
    "        (pandas.DataFrame) qtl results with ABF, PP, and credible sets computed\n",
    "    \"\"\"\n",
    "    # some feature QTL stats may also have multiple results per variants \n",
    "    # so need to reduce or remove these\n",
    "    # these are typically a results of variants that are multi-allelic like indels\n",
    "    trait_df = trait_df.drop_duplicates(subset=['variant_id'], keep='first')\n",
    "    # calculate the ABF's for the feature's QTL results\n",
    "    trait_df[\"logABF\"] = trait_df.apply(\n",
    "    lambda result: clc.calc_abf(pval=result.p_value, maf=clc.freq_to_maf(result.maf),\n",
    "                                n=int(result.Weight)), \n",
    "                                axis=1)    \n",
    "    trait_df = trait_df.sort_values(\"logABF\", ascending=False)\n",
    "    # calculate the posterior probability for each variant\n",
    "    trait_df['PP'] = clc.compute_pp(trait_df.logABF)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(trait_df)\n",
    "    # subset the feature QTL variants to just those present in the GWAS\n",
    "    trait_df = trait_df.loc[trait_df.variant_id.isin(other_stats.variant_id)] \n",
    "    return trait_df\n",
    "\n",
    "def process_gwas(trait_stats: DataFrame, other_stats: DataFrame) -> DataFrame:\n",
    "    \"\"\" Prep the risk (or trait1) results for use in colocalization. \n",
    "        Where prep performs subet of variant to those present in other (trait2/qtl),\n",
    "        Wakefield Approx Bayes Factor, posterior probabliltiy,\n",
    "        and credible sets calculations and identification\n",
    "    Args:\n",
    "        trait_stats (pandas.DataFrame) trait1 (or risk) stats to be used with these\n",
    "            qtl (or trait2) stats for colocalization\n",
    "        other_stats (pandas.DataFrame) trait2 (or qtl) stats to be used with these\n",
    "            risk (or trait2) stats for colocalization\n",
    "    Returns:\n",
    "        (pandas.DataFrame) risk results with ABF, PP, and credible sets computed\n",
    "    \"\"\" \n",
    "    # subset the risk summary stats by the feature's QTL variants present\n",
    "    ret_df = trait_stats.loc[trait_stats.variant_id.isin(other_stats.variant_id)].copy()\n",
    "    # calculate the ABF's for the risk results\n",
    "    ret_df['logABF'] = ret_df.apply(\n",
    "        lambda result: clc.calc_abf(pval=result.p_value, \n",
    "                                    maf=clc.freq_to_maf(result.effect_allele_frequency),\n",
    "                                    n=result.n_total, \n",
    "                                    prop_cases=result.case_prop), axis=1)\n",
    "    ret_df = ret_df.sort_values('logABF', ascending=False)  \n",
    "    # calculate the posterior probability for each variant\n",
    "    ret_df['PP'] = clc.compute_pp(ret_df.logABF)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(ret_df)\n",
    "    return ret_df\n",
    "\n",
    "def ensure_matched_indices(df1: DataFrame, df2: DataFrame) -> {DataFrame, DataFrame}:\n",
    "    \"\"\" make sure the two datasets are ordered the same\n",
    "        modifies both df1 and df2\n",
    "    Args:\n",
    "        df1 (pandas.DataFrame) risk or trait1 data\n",
    "        df2 (pandas.DataFrame) qtl or trait2 data\n",
    "    \"\"\" \n",
    "    # ensure that the risk and feature variants ABF's are ordered the same\n",
    "    df1 = df1.set_index('variant_id')\n",
    "    df2 = df2.set_index('variant_id')\n",
    "    shared_indices = df1.index.intersection(df2.index)\n",
    "    df1 = df1.loc[shared_indices,]\n",
    "    df2 = df2.loc[shared_indices,]\n",
    "    return df1, df2\n",
    "\n",
    "def colocalize(t1_abfs, t2_abfs, feature: str) -> Series:\n",
    "    \"\"\" Perform the colocalization between trait1 and trait2 ABFs\n",
    "    Args:\n",
    "        t1_abfs (array_like) trait1's ABFs\n",
    "        t2_abfs (array_like) trait2's ABFs\n",
    "        feature (string) trait2's name or ID\n",
    "    Returns:\n",
    "        (pandas.Series) named colocalization posterior probabilities\n",
    "    \"\"\"\n",
    "    h_probs = clc.combine_abf(t1_abfs, t2_abfs)\n",
    "    names = [f'H{x}' for x in range(5)]\n",
    "    cl_result = Series(data=around(h_probs, decimals=3), index=names)\n",
    "    cl_result['feature'] = feature\n",
    "    return cl_result  \n",
    "\n",
    "def compute_combined_pp(t1_df: DataFrame, t2_df: DataFrame) -> DataFrame:\n",
    "    \"\"\" Compute the the combined ABFs posterior probabilities and credible sets\n",
    "    Args:\n",
    "        t1_df (pandas.DataFrame) risk or trait1's data\n",
    "        t2_df (pandas.DataFrame) qtl or trait2's data\n",
    "    Returns:\n",
    "        (pandas.DataFrame) t1_df combined with t2_df with PP and credible sets ID'd\n",
    "    \"\"\"\n",
    "    ret_df = merge(t1_df, t2_df, how='inner', on='variant_id', suffixes=('_risk', '_qtl'))\n",
    "    # calculate the posterior probability for each variant\n",
    "    ret_df['PP'] = clc.compute_pp(ret_df.logABF_risk + ret_df.logABF_qtl)\n",
    "    # identify the credible set(s), 95% and 99%, the the posterior probabilities\n",
    "    clc.credible_sets(ret_df)\n",
    "    ret_df = ret_df.rename(columns={'PP': 'h4_pp'})\n",
    "    return ret_df\n",
    "\n",
    "def process_modality(name: str, in_dir: str, variants: list, \n",
    "                     out_dir: str, gwas_stats_df: DataFrame, dx: str):\n",
    "    qtl_full_df = load_meta_eqtl_results(in_dir, name)\n",
    "    traits_oi = find_intersecing_traits(qtl_full_df, variants, name)\n",
    "    coloc_scores = []\n",
    "    coloc_h4_pps = None\n",
    "    support_cnt = 0\n",
    "    # process data by chromosome\n",
    "    for feature in traits_oi:\n",
    "        feature_df = qtl_full_df.loc[qtl_full_df.trait == feature].copy()\n",
    "        feature_df = process_qtl(feature_df, gwas_stats_df)\n",
    "        # prep the GWAS results\n",
    "        risk_df = process_gwas(gwas_stats_df, feature_df)\n",
    "        # ensure that the risk and feature variants ABF's are ordered the same\n",
    "        risk_df, feature_df = ensure_matched_indices(risk_df, feature_df)\n",
    "        # perform the colocalization\n",
    "        cl_result = colocalize(risk_df.logABF, feature_df.logABF, feature)\n",
    "        # if H4 is supported then compute H4.PP the H4 credible sets\n",
    "        cl_result['h4_supported'] = clc.h4_supported(cl_result)\n",
    "        if cl_result.H4 >= min_h4:\n",
    "            combined_df = compute_combined_pp(risk_df, feature_df)\n",
    "            coloc_h4_pps = concat([coloc_h4_pps, combined_df])\n",
    "            support_cnt += 1\n",
    "        # add these scores to the rest\n",
    "        coloc_scores.append(cl_result)\n",
    "    # create a dataframe from the list of coloc scores    \n",
    "    coloc_scores_df = DataFrame(coloc_scores)\n",
    "    ### save the result files for this cell-type\n",
    "    coloc_scores_files = f'{out_dir}/{name}_{dx}.coloc.pp.csv'\n",
    "    coloc_casuals_files = f'{out_dir}/{name}_{dx}.casuals.pp.parquet'\n",
    "    coloc_scores_df.to_csv(coloc_scores_files, index=False)\n",
    "    if not coloc_h4_pps is None:\n",
    "        coloc_h4_pps.to_parquet(coloc_casuals_files)\n",
    "        print(f'{name} found H4 support at {support_cnt} traits')\n",
    "    else:\n",
    "        print(f'{name} no H4 supported so no coloc_h4_pps')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da59a4-7894-4815-a270-00acb4dbc68f",
   "metadata": {},
   "source": [
    "### load the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db625f4b-f5bf-4bc8-be25-159e68fcc22a",
   "metadata": {},
   "source": [
    "#### load the risk variants of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b94129-b8cd-4fa4-a074-878e0d7c3b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 1)\n",
      "CPU times: user 5.38 ms, sys: 54 Âµs, total: 5.44 ms\n",
      "Wall time: 4.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "variants_oi_df = read_csv(index_variants_file)\n",
    "print(variants_oi_df.shape)\n",
    "index_variants = list(variants_oi_df.variant.unique())\n",
    "if DEBUG:\n",
    "    display(variants_oi_df.head())\n",
    "    print(index_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d9e65-921e-478a-91b6-50fea44c2363",
   "metadata": {},
   "source": [
    "#### load the full gwas summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cfc367d-2255-4383-abfe-f3baff4afb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7769022, 12)\n",
      "CPU times: user 10.8 s, sys: 796 ms, total: 11.6 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gwas_stats_df = read_csv(gwas_sum_stats_file, sep='\\t')\n",
    "print(gwas_stats_df.shape)\n",
    "if DEBUG:\n",
    "    display(gwas_stats_df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b11de-af9e-423d-bd67-613300dce68d",
   "metadata": {},
   "source": [
    "#### set case proportion for GWAS summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308fb6ef-6753-4c40-b319-a2d84d25702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_stats_df['n_total'] = gwas_stats_df.n_cases + gwas_stats_df.n_controls\n",
    "    \n",
    "gwas_stats_df['case_prop'] = gwas_stats_df.n_cases / gwas_stats_df.n_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ffbf7-0d74-41bd-a646-a0a4d865d2b6",
   "metadata": {},
   "source": [
    "#### subset index variant stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cbf6db5-d58a-4b79-a2b4-e321a26ee1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 13)\n"
     ]
    }
   ],
   "source": [
    "index_stats_df = gwas_stats_df.loc[gwas_stats_df.variant_id.isin(index_variants)]\n",
    "print(index_stats_df.shape)\n",
    "if DEBUG:\n",
    "    display(index_stats_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18363b73-04a6-4818-8232-d4c15025155c",
   "metadata": {},
   "source": [
    "### load data, format data, and save new input files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d1bc6-4133-4eac-8f4b-8c533f9c1be9",
   "metadata": {},
   "source": [
    "#### meta eQTL result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2f92e-95e1-432f-ba2d-cd89581a5242",
   "metadata": {},
   "source": [
    "#### load and analyze meta eQTL results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfc3b904-18ae-4efc-8273-f95f5578f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### ('Bulk-meta', 'foundin_daNA_Bulk-meta') ####\n",
      "#### ('DAn-meta', 'foundin_daNA_DAn-meta') ####\n",
      "shape of foundin_daNA_Bulk-meta results (17569095, 12)\n",
      "shape of foundin_daNA_DAn-meta results (17598904, 12)\n",
      "full qtl for 4467 features for foundin_daNA_DAn-meta\n",
      "foundin_daNA_DAn-meta intersect shape (33, 12)\n",
      "foundin_daNA_DAn-meta number of traits 32\n",
      "full qtl for 5753 features for foundin_daNA_Bulk-meta\n",
      "foundin_daNA_Bulk-meta intersect shape (43, 12)\n",
      "foundin_daNA_Bulk-meta number of traits 42\n",
      "foundin_daNA_DAn-meta found H4 support at 17 traits\n",
      "foundin_daNA_Bulk-meta found H4 support at 24 traits\n",
      "CPU times: user 22.5 ms, sys: 39.2 ms, total: 61.7 ms\n",
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "jobs = {}\n",
    "for modality in modalities:\n",
    "    set_name = set_name_frmt.format(modality=modality)\n",
    "    print(f'#### {modality, set_name} ####')\n",
    "    p = Process(target=process_modality, \n",
    "                args=(set_name, meta_dir, index_variants, results_dir,\n",
    "                      gwas_stats_df, dx))\n",
    "    p.start()\n",
    "    # Append process and key to keep track\n",
    "    jobs[modality] = p    \n",
    "# Wait for all processes to finish\n",
    "for key, p in jobs.items():\n",
    "    p.join()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07921397-973a-4531-86fa-e2cba64b79ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 23 17:18:38 EDT 2024\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
