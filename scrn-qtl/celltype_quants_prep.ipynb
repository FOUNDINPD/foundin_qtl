{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to prep quantified features data\n",
    "detection, sex check, normalization, and covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import libraries and notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer, MinMaxScaler\n",
    "import random\n",
    "from umap import UMAP\n",
    "import ppscore as pps\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy.stats as stats\n",
    "import concurrent.futures\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameter variables\n",
    "cohort = 'foundin'\n",
    "cell_type = 'DA'\n",
    "day = 'da65'\n",
    "quant_type = 'scrn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming\n",
    "cohort_build = f'{cohort}.{day}.{cell_type}'\n",
    "\n",
    "# directories\n",
    "wrk_dir = f'/home/jupyter/sceqtl'\n",
    "quants_dir = f'{wrk_dir}/quants'\n",
    "info_dir = f'{wrk_dir}/sample_info'\n",
    "\n",
    "# input files\n",
    "src_quants_matrix = f'{quants_dir}/{cohort_build}.{quant_type}.hdf5'\n",
    "covariates_file = f'{info_dir}/{cohort}_{quant_type}_sample_info.csv'\n",
    "features_file = f'{quants_dir}/gencode_v29.lncipedia_v5_2_hc.annotation.pkl'\n",
    "# gene file of genes to exclude from variance detection\n",
    "# using Human Protein Atlas 'elevated' gene set for braim\n",
    "tissue_genes_file = f'{quants_dir}/tissue_category_rna_brain_Tissue.tsv'\n",
    "\n",
    "# output files\n",
    "umap_covs_file = f'{info_dir}/{cohort_build}.umap.covs.csv'\n",
    "scaled_quants_file = f'{quants_dir}/{cohort_build}.scaled.hdf5'\n",
    "adj_quants_file = f'{quants_dir}/{cohort_build}.scaled.adj.hdf5'\n",
    "tnsrqtl_pheno_file = f'{quants_dir}/{cohort_build}.scaled.adj.bed.gz'\n",
    "\n",
    "# constant values\n",
    "max_missing_rate = 0.25\n",
    "min_ppscore = 0.05\n",
    "min_pearson = 0.22\n",
    "\n",
    "repeated_id_dict = {'PPMI3966B1': 'PPMI3966', 'PPMI3966B2': 'PPMI3966', \n",
    "                    'PPMI3966B3': 'PPMI3966', 'PPMI3966B5': 'PPMI3966',\n",
    "                    'PPMI3966B5E6': 'PPMI3966', 'PPMI3966B5E8': 'PPMI3966'}\n",
    "# to match geno's use PPMI3966 Batch3\n",
    "replace_id_dict = {'PPMI3966B3': 'PPMI3966'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notebook functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions for detection rates calculations and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_detection_rates(this_df, quant_type, round_percision=1, \n",
    "                              min_quant_value=None):\n",
    "    if min_quant_value is None:\n",
    "        min_quant_value = this_df.round(round_percision).min().min()\n",
    "\n",
    "    print(f'minimun {quant_type} value is {min_quant_value}')\n",
    "\n",
    "    detected_df = this_df.mask(this_df.round(round_percision) <= min_quant_value, 0)\n",
    "\n",
    "    # calculate the missing counts from the detected df mask\n",
    "    trait_missing_rates = round(detected_df.isin({0}).sum(0)/detected_df.shape[0], 2)\n",
    "    sample_missing_rates = round(detected_df.isin({0}).sum(1)/detected_df.shape[1], 2)\n",
    "\n",
    "    print(f'{len(trait_missing_rates)} features with mean missing \\\n",
    "rate = {trait_missing_rates.mean()}')\n",
    "    print(f'{len(sample_missing_rates)} samples with mean missing \\\n",
    "rate = {sample_missing_rates.mean()}')\n",
    "    return trait_missing_rates, sample_missing_rates\n",
    "\n",
    "def plot_missing_rates(feature_rates, sample_rates):\n",
    "    sns.set()\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.distplot(feature_rates.values)\n",
    "    plt.title('Features missingness rates')\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.distplot(sample_rates.values)\n",
    "    plt.title('Samples missingness rates')\n",
    "    plt.show()\n",
    "    \n",
    "def bad_callrate_features(features_missing_rates, max_missing_rate):\n",
    "    bad_call_rates = features_missing_rates[features_missing_rates > max_missing_rate]\n",
    "    print(f'features with bad call rates shape {bad_call_rates.shape}, \\\n",
    "fraction of features with bad rates {bad_call_rates.shape[0]/features_missing_rates.shape[0]}')\n",
    "    return bad_call_rates\n",
    "\n",
    "def subset_well_detected_features(this_df, bad_call_rates):\n",
    "    detected_traits = list(set(this_df.columns)-set(bad_call_rates.index))\n",
    "    this_wd_df = this_df[detected_traits]\n",
    "    print(f'shape of well detected quants {this_wd_df.shape}')\n",
    "    return this_wd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to generate and visualize known and unknow covariates using UMAP and PPScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting umap of traits with covar high lights\n",
    "def plot_umap_clusters(umap_df, hue_cov=None, style_cov=None, size_cov=None):\n",
    "    # umap_plot_file = f'{WRKDIR}/{COHORTBUILD}.umap.residuals.umap.plot.png'\n",
    "    sns.set()\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    sns_plot = sns.scatterplot(x='x_umap',y='y_umap', \\\n",
    "                               hue=hue_cov, style=style_cov, size=size_cov, \\\n",
    "                               data=umap_df)\n",
    "    plt.xlabel('x-umap')\n",
    "    plt.ylabel('y-umap')\n",
    "    #plt.legend(loc='lower right', prop={'size': 6})\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0,prop={'size': 10})\n",
    "    # plt.savefig(umap_plot_file,format='png',dpi=600,bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# small function to generate umap from pandas dataframe, for all features (columns) \\\n",
    "# and return back as dataframe with source index intact\n",
    "def generate_umap_covs_df(this_df, other_covs_df=None, \n",
    "                             rnd_digits=3, merge_input=False):\n",
    "    #run UMAP on the data frame features\n",
    "    umap_results = UMAP(random_state=42).fit_transform(this_df)\n",
    "    umap_df = pd.DataFrame(umap_results,columns=['x_umap','y_umap'], \\\n",
    "                                       index=this_df.index).round(rnd_digits)\n",
    "    if merge_input:\n",
    "        umap_df = umap_df.merge(this_df,left_index=True,right_index=True)\n",
    "    if other_covs_df is not None:\n",
    "        umap_df = umap_df.merge(other_covs_df, how='left', \n",
    "                                left_index=True, right_index=True)\n",
    "    print(f'The dimensions of the umap df and the traits are {umap_df.shape}')\n",
    "    return umap_df \n",
    "\n",
    "# function to iterate over target features and use PPScore to find covarites of interest\n",
    "def pps_predict_targets(this_df, target_list):\n",
    "    covs_to_check = []\n",
    "#     covs_list = ['x_umap', 'y_umap']\n",
    "    for this_cov in target_list:\n",
    "        print(this_cov)\n",
    "        predictors_df = pps.predictors(this_df, this_cov)\n",
    "        # drop anything that has ppscore of zero\n",
    "        predictors_df = predictors_df.loc[predictors_df['ppscore'] > min_ppscore]\n",
    "        display(predictors_df)\n",
    "        covs_to_check.extend(list(predictors_df['x'].values))\n",
    "\n",
    "    print(f'found {len(covs_to_check)} covariates that may preditct target covariates')    \n",
    "    return covs_to_check\n",
    "\n",
    "# plot ppscore matrix \n",
    "def plot_ppscore_matrix(this_df, covs_to_check, cov_targets):\n",
    "    matrix_df = pps.matrix(this_df[(set(covs_to_check) | set(cov_targets))])\n",
    "    matrix_df = matrix_df.loc[matrix_df['ppscore'] > min_ppscore]\n",
    "    print(matrix_df.shape)\n",
    "\n",
    "    matrix_df['ppscore'] = matrix_df['ppscore'].round(2)\n",
    "    plot_matrix_df = matrix_df[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n",
    "    print(plot_matrix_df.shape)\n",
    "    # display(plot_matrix_df)\n",
    "\n",
    "    plt.figure(figsize=(plot_matrix_df.shape[0],plot_matrix_df.shape[1])) \n",
    "    sns.heatmap(plot_matrix_df, vmin=0, vmax=1, cmap='Blues', linewidths=0.05, \n",
    "                annot=True, annot_kws={'fontsize':12})\n",
    "    plt.title('PPScore heatmap')\n",
    "    plt.show()\n",
    "    \n",
    "# plot heatmap of Pearson correlation matrix for PPScore covariates\n",
    "def plot_correlation_heatmap(this_df, covs_list : list=None):\n",
    "    sns.set()\n",
    "    cor = this_df.corr(method='pearson')\n",
    "    cor.dropna(how='all', inplace=True)\n",
    "    modified_title = ''\n",
    "    if covs_list is not None:\n",
    "        \n",
    "        limited_cor = cor[covs_list]\n",
    "        cor = limited_cor.loc[(limited_cor['x_umap'].abs() > min_pearson) | \n",
    "                              (limited_cor['y_umap'].abs() > min_pearson)]\n",
    "        modified_title = 'limited'\n",
    "    print(cor.shape)\n",
    "    fig_width = cor.shape[1] if cor.shape[1] > 12 else 12\n",
    "    fig_height = cor.shape[0] if cor.shape[1] > 12 else 12\n",
    "    plt.figure(figsize=(fig_width, fig_height))        \n",
    "    sns.heatmap(cor[(cor > min_pearson) | (cor < -min_pearson)], annot=True, \n",
    "                annot_kws={\"fontsize\":10}, linewidths=0.05, cmap='Blues')    \n",
    "    plt.title(f'Pearson heatmap of PPScore covariates {modified_title}')\n",
    "    plt.show()\n",
    "\n",
    "# function to one-hot encode the categorical covariates and merge with continuous ones    \n",
    "def dummy_covs_as_needed(this_df):\n",
    "    temp_df = this_df.copy()\n",
    "    cats_df = temp_df.select_dtypes(include=['object'])\n",
    "    print(f'categoricals shape {cats_df.shape}')\n",
    "    dums_df = pd.get_dummies(cats_df)\n",
    "    print(f'one-hot encoded categoricals shape {dums_df.shape}')\n",
    "\n",
    "    temp_df = temp_df.merge(dums_df, how='inner', left_index=True, right_index=True)\n",
    "    print(f'new covs df shape {temp_df.shape}')\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small function to plot before and after of transform based on named feature,\n",
    "# or if a feature isn't specified then one pull at random\n",
    "def plot_trnsfrm_effect_example(before_df, after_df, feature_id=None,\n",
    "                                bf_label='quantile transformed', \n",
    "                                af_label='quantile transformed and covariate adjusted'):\n",
    "    # if no feature ID provided get randome one\n",
    "    if feature_id is None:\n",
    "        feature_id = random.sample(list(after_df.columns), 1)[0]\n",
    "    \n",
    "    sns.distplot(before_df[feature_id])\n",
    "    plt.title(f'{feature_id} {bf_label}')\n",
    "    plt.show()\n",
    "    sns.distplot(after_df[feature_id])\n",
    "    plt.title(f'{feature_id} {af_label}')\n",
    "    plt.show()\n",
    "    sns.scatterplot(x=before_df[feature_id], y=after_df[feature_id])\n",
    "    plt.title(f'{feature_id}')\n",
    "    plt.xlabel(f'{bf_label}')\n",
    "    plt.ylabel(f'{af_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small function to perform the quantile transform and minmax scale on a pandas dataframe\n",
    "def scale_dataframe(this_df : pd.DataFrame):\n",
    "    scaledX = MinMaxScaler().fit_transform(QuantileTransformer(output_distribution='normal')\n",
    "                                           .fit_transform(this_df))\n",
    "    scaled_df = pd.DataFrame(data=scaledX, columns=this_df.columns, \n",
    "                                 index=this_df.index)  \n",
    "    return scaled_df    \n",
    "\n",
    "# exclude low variance features from covariate generation\n",
    "def exclude_low_var_features(this_df: pd.DataFrame, quartile_to_drop: str ='25%', \n",
    "                             known_feature_to_drop=None):\n",
    "    quants_vars = this_df.var() \n",
    "    print(quants_vars.describe())\n",
    "    # drop features within the lower quartile of variance\n",
    "    min_variance = quants_vars.describe()['25%']\n",
    "    # min_variance = quants_vars.describe()['50%']\n",
    "    keep = quants_vars[quants_vars > min_variance]\n",
    "    if known_feature_to_drop is not None:\n",
    "        keep_ids = set(keep.index) - set(known_feature_to_drop)\n",
    "    else:\n",
    "        keep_ids = set(keep.index)\n",
    "    quants_wd_var_df = this_df[keep_ids]\n",
    "    print(f'shape of the features to keep {keep.shape}')\n",
    "    print(f'shape of input features df {this_df.shape}')\n",
    "    print(f'shape of variance features df {quants_wd_var_df.shape}')\n",
    "    return quants_wd_var_df\n",
    "\n",
    "# function to fit linear model to covariates and calculate the standardized residuals\n",
    "def covariate_residuals(traits_df, covars_df):\n",
    "    lm = LinearRegression(n_jobs=16)\n",
    "    residuals_df = traits_df.copy()\n",
    "    covar_scores_by_trait = {}\n",
    "\n",
    "    for trait in traits_df:\n",
    "            model = lm.fit(covars_df, traits_df[trait])\n",
    "            covar_scores_by_trait[trait] = model.score(covars_df,traits_df[trait])\n",
    "            model_predicted = model.predict(covars_df)\n",
    "            residuals_df[trait] = stats.zscore(traits_df[trait] - model_predicted)\n",
    "            \n",
    "#     # We can use a with statement to ensure threads are cleaned up promptly\n",
    "#     with concurrent.futures.ProcessPoolExecutor() as ppe:\n",
    "#         # Start the load operations and mark each future with its URL\n",
    "#         future_to_residual = {executor.submit(compute_residuals, trait): trait for trait in traits_df}\n",
    "#         for future in concurrent.futures.as_completed(future_to_residual):\n",
    "#             covar_scores_by_trait[trait], residuals_df[trait] = future_to_residual[future]\n",
    "\n",
    "    # scale the residuals\n",
    "    residualsX = MinMaxScaler().fit_transform(residuals_df)\n",
    "    residuals_df = pd.DataFrame(data=residualsX, columns=traits_df.columns, \n",
    "                                index=traits_df.index)\n",
    "\n",
    "    # grab the covariates model scores\n",
    "    covar_scores_by_trait_df = pd.DataFrame.from_dict(covar_scores_by_trait,\n",
    "                                                      columns=['score'],\n",
    "                                                      orient='index').round(3)\n",
    "    covar_scores_by_trait_df.index.name = 'featureID'\n",
    "    return residuals_df, covar_scores_by_trait_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### input output functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small function to save hdf file\n",
    "def write_df_to_hdf(this_df, file_name, key='quants', mode='w'):\n",
    "    this_df.to_hdf(file_name, key=key, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load covariates files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs_df = pd.read_csv(covariates_file, index_col=0)\n",
    "# drop any duplicated indices\n",
    "print(covs_df.shape)\n",
    "covs_df = covs_df[~covs_df.index.duplicated(keep='first')]\n",
    "print(covs_df.shape)\n",
    "# display(covs_df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the feature annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "features_df = pd.read_pickle(features_file)\n",
    "print(features_df.shape)\n",
    "# display(features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the quantified features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "quants_df = pd.read_hdf(src_quants_matrix, index_col=0)\n",
    "print(quants_df.shape)\n",
    "# display(quants_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split assayid into meta data bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_parts = quants_df.index.str.split('_', expand=True).to_frame()\n",
    "id_parts.columns = ['assay', 'sampleid', 'cdi', 'day']\n",
    "id_parts['assayid'] = quants_df.index\n",
    "print(id_parts.shape)\n",
    "# display(id_parts.sample(5))\n",
    "id_parts['sampleid'].replace(repeated_id_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### capture the assayid to wgsid for formatting phenotypes for use with wgs genotypes later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = id_parts[['sampleid', 'assayid']]\n",
    "id_map['sampleid'].replace(replace_id_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check to see if missing covariate info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(id_parts['sampleid']) - set(covs_df['PPMI_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for merging known covariates with umaps will need to add cell labelled assay ids into covariates dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_parts.reset_index(inplace=True)\n",
    "id_parts.drop(columns=['level_0', 'level_1', 'level_2', 'level_3'], inplace=True)\n",
    "covs_df = covs_df.merge(id_parts, left_on='PPMI_ID', right_on='sampleid')\n",
    "covs_df.index = covs_df['assayid']\n",
    "covs_df = covs_df[~covs_df.index.duplicated(keep='first')]\n",
    "print(covs_df.shape)\n",
    "# display(covs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any unexpected samples; ie probably name frmt issue\n",
    "set(quants_df.index) - set(covs_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check expected sex of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vawter MP, Evans S, Choudary P et al. Gender-specific gene expression in \n",
    "#post-mortem human brain: localization to sex chromosomes. \n",
    "#Neuropsychopharmacology 2004;29:373–84.\n",
    "\n",
    "sex_specific_genes = ['XIST','RPS4Y1','RPS4Y2','KDM5D','UTY','DDX3Y','USP9Y']\n",
    "sex_genes_present = list(set(sex_specific_genes) & set(quants_df.columns))\n",
    "quants_sex_df = quants_df[sex_genes_present]\n",
    "print(quants_sex_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sex_umap_df = generate_umap_covs_df(quants_sex_df, covs_df)\n",
    "plot_umap_clusters(sex_umap_df, hue_cov='sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sex_umap_df.loc[(sex_umap_df['x_umap'] < 0) & (sex_umap_df['sex'] == 'Female')].shape)\n",
    "print(sex_umap_df.loc[(sex_umap_df['x_umap'] > 0) & (sex_umap_df['sex'] == 'Male')].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### double check the age range to make sure no young subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs_df['age_at_baseline'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate, plot detection rates and subset well detected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trait_miss_rates, sample_miss_rates = calculate_detection_rates(quants_df, quant_type)\n",
    "plot_missing_rates(trait_miss_rates, sample_miss_rates)\n",
    "bad_call_rate_features = bad_callrate_features(trait_miss_rates, max_missing_rate)\n",
    "quants_wd_df = subset_well_detected_features(quants_df, bad_call_rate_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standardize the full dataset using quantile transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "traits_scaled_df = scale_dataframe(quants_wd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trnsfrm_effect_example(quants_df, traits_scaled_df,\n",
    "                            bf_label=quant_type, \n",
    "                            af_label='quantile transformed & scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save quantile standardized, well detected data for all days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df_to_hdf(traits_scaled_df, scaled_quants_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exclude low variance features from covariate generation exclude tissue elelvated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quants_var_df = exclude_low_var_features(traits_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_features_df = pd.read_csv(tissue_genes_file, sep='\\t')\n",
    "print(tissue_features_df.shape)\n",
    "# display(tissue_features_df.head())\n",
    "\n",
    "variance_features = set(quants_var_df.columns) - (set(sex_specific_genes) | set(tissue_features_df['Gene']))\n",
    "print(len(variance_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take a look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate unknown covariates and see if know covariates are source of variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "umap_df = generate_umap_covs_df(quants_var_df[variance_features], covs_df)\n",
    "covs_target_list = ['x_umap', 'y_umap']\n",
    "covs_to_check = pps_predict_targets(umap_df, covs_target_list)\n",
    "plot_ppscore_matrix(umap_df, covs_to_check, covs_target_list)\n",
    "umap_dums_covs_df = dummy_covs_as_needed(umap_df[(set(covs_to_check) | \n",
    "                                                  set(covs_target_list))])\n",
    "plot_correlation_heatmap(umap_dums_covs_df)\n",
    "plot_correlation_heatmap(umap_dums_covs_df, covs_target_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot umap of with known covariates of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap_clusters(umap_df, hue_cov='Batch', size_cov='EstimatedNumberofCells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap_clusters(umap_df, hue_cov='RECRUITMENT_CAT', size_cov='TotalGenesDetected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keep created covars and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the covariates\n",
    "umap_covs_df = scale_dataframe(umap_df[covs_target_list])\n",
    "# now save the covariates\n",
    "umap_covs_df.to_csv(umap_covs_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### covariate adjust the normalized data by the covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see in df's have same indices\n",
    "if not traits_scaled_df.index.equals(umap_covs_df.index):\n",
    "    print('indices are not equal re-index')\n",
    "    umap_covs_df.reindex(traits_scaled_df.index)\n",
    "    \n",
    "traits_scaled_df.index.equals(umap_covs_df.index)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "residuals_df, cov_scores_df = covariate_residuals(traits_scaled_df, umap_covs_df)\n",
    "\n",
    "#take a peek at the data\n",
    "print(residuals_df.shape)\n",
    "print(cov_scores_df.shape)\n",
    "\n",
    "# print(cov_scores_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a summary of the covariates model scores\n",
    "print(cov_scores_df.describe())\n",
    "# look at the distribution of covariate model scores, \n",
    "# ie get a sense any feature driven by covariates\n",
    "sns.set()\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.distplot(cov_scores_df['score'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save quantile normalized and covariate adjusted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "residuals_df.to_hdf(adj_quants_file, key='quants', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take a look at the normalized and covariate adjusted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trnsfrm_effect_example(traits_scaled_df, residuals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find feature with largest score\n",
    "large_adj_trait = cov_scores_df.loc[cov_scores_df['score'] == max(cov_scores_df['score'])]\n",
    "print(large_adj_trait)\n",
    "large_adj_traid_id = large_adj_trait.index.values[0]\n",
    "\n",
    "# spot check same feature with largest adjustment effect\n",
    "plot_trnsfrm_effect_example(traits_scaled_df, residuals_df, large_adj_traid_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what are the post normalization and covariate adjusted umap variables correlated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "umap_df = generate_umap_covs_df(residuals_df, covs_df)\n",
    "covs_to_check = pps_predict_targets(umap_df, covs_target_list)\n",
    "plot_ppscore_matrix(umap_df, covs_to_check, covs_target_list)\n",
    "# umap_dums_covs_df = dummy_covs_as_needed(umap_df[(set(covs_to_check) | \n",
    "#                                                   set(covs_target_list))])\n",
    "# plot_correlation_heatmap(umap_dums_covs_df)\n",
    "# plot_correlation_heatmap(umap_dums_covs_df, covs_target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap_clusters(umap_df, hue_cov='Batch', size_cov='EstimatedNumberofCells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap_clusters(umap_df, hue_cov='RECRUITMENT_CAT', size_cov='TotalGenesDetected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### since switching to tensorQTL can just use one large transcriptome pheno bed instead of per chrom pheno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# get feature annots for present features\n",
    "feature_present_df = features_df.loc[features_df['gene_name'].isin(residuals_df.columns)]\n",
    "# tensorQTL pheno bed is rows = features and columns = samples\n",
    "# where first four columns are chr, start, end, phenotype_id, then sample1 ... sampleN\n",
    "\n",
    "# create dict for renaming columns (samples) from assayid to geno_id\n",
    "sample_col_dict = id_map.set_index('assayid').to_dict()['sampleid']\n",
    "\n",
    "# transpose the residuals df from sample x feature to feature x sample\n",
    "tresiduals_df = residuals_df.transpose()\n",
    "\n",
    "# modify annots\n",
    "feature_present_df = feature_present_df[['seqname', 'start', 'end', 'gene_name', 'strand']].copy()\n",
    "feature_present_df.rename(columns={'seqname': 'chr', 'start': 'fstart', \n",
    "                                   'end': 'fend'}, inplace=True)\n",
    "# for tensorQTL 'end' column is TSS so set appropriately\n",
    "feature_present_df['end'] = np.where(feature_present_df['strand'] == '+',  \n",
    "                                     feature_present_df['fstart'], \n",
    "                                     feature_present_df['fend'])\n",
    "feature_present_df['start'] = feature_present_df['end'] - 1\n",
    "# there is a feature per transcript, so can be multiple entries per feature, so just keep longest\n",
    "feature_present_df['length'] = feature_present_df['fend'] - feature_present_df['fstart']\n",
    "feature_present_df.sort_values(by=['gene_name', 'length'], \n",
    "                               inplace=True, ascending=False)\n",
    "print(feature_present_df.shape)\n",
    "feature_present_df.drop_duplicates(subset=['gene_name'], keep='first', \n",
    "                                   inplace=True, ignore_index=True)\n",
    "feature_present_df.set_index('gene_name', inplace=True, drop=False)\n",
    "feature_present_df = feature_present_df.reindex(tresiduals_df.index)\n",
    "\n",
    "# insert the feature annots\n",
    "tresiduals_df.insert( 0, column='chr', value=feature_present_df['chr'])\n",
    "tresiduals_df.insert( 1, column='start', value=feature_present_df['start'])\n",
    "tresiduals_df.insert( 2, column='end', value=feature_present_df['end'])\n",
    "tresiduals_df.insert( 3, column='phenotype_id', value=feature_present_df['gene_name'])\n",
    "\n",
    "# if there are any genes that were in quants but not feature annots\n",
    "# remove these with missing positions\n",
    "tresiduals_df = tresiduals_df.loc[~tresiduals_df['chr'].isna()]\n",
    "# make the positions ints instead of floats\n",
    "tresiduals_df['start'] = tresiduals_df['start'].astype('int64')\n",
    "tresiduals_df['end'] = tresiduals_df['end'].astype('int64')\n",
    "\n",
    "# now rename sample ids in columns\n",
    "tresiduals_df.rename(columns=sample_col_dict, inplace=True)\n",
    "\n",
    "tresiduals_df.to_csv(tnsrqtl_pheno_file, index=False, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
